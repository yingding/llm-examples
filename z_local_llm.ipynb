{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "this notebook demos example of using llm in a MPS backend (apple silicon GPU) using torch 2.x\n",
    "\n",
    "Referece:\n",
    "* torch 2.x MPS Backend: https://pytorch.org/docs/stable/notes/mps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# check that MPS is availabe (Metal Performance Shaders)\n",
    "if not torch.backends.mps.is_available():\n",
    "    print(\"MPS is not available\")\n",
    "else:\n",
    "    print(\"MPS is available\")\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(mps_device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DirectorySetting:\n",
    "    \"\"\"set the directory for the model download\"\"\"\n",
    "    home_dir: str=\"/home/jovyan/llm-models\"\n",
    "    transformers_cache_home: str=\"core-kind/yinwang/models\"\n",
    "    huggingface_token_file: str=\"core-kind/yinwang/.cache/huggingface/token\"\n",
    "\n",
    "    def get_cache_home(self):\n",
    "        \"\"\"get the cache home\"\"\"\n",
    "        return f\"{self.home_dir}/{self.transformers_cache_home}\"\n",
    "    \n",
    "    def get_token_file(self):\n",
    "        \"\"\"get the token file\"\"\"\n",
    "        return f\"{self.home_dir}/{self.huggingface_token_file}\"\n",
    "    \n",
    "dir_mode_map = {\n",
    "    \"kf_notebook\": DirectorySetting(),\n",
    "    \"mac_local\": DirectorySetting(home_dir=\"/Users/yingding\", transformers_cache_home=\"MODELS\", huggingface_token_file=\"MODELS/.huggingface_token\"),\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\"\n",
    "\n",
    "dir_setting = dir_mode_map[default_dir_mode]\n",
    "\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ['XDG_CACHE_HOME'] = dir_setting.get_cache_home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yingding/MODELS'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['XDG_CACHE_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.1\n",
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, default_model_type)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast tokenizer\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/23889#issuecomment-1584090357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n"
     ]
    }
   ],
   "source": [
    "# MAX_POSITION_EMBEDDINGS = 3072\n",
    "# MAX_LENGTH = 4096\n",
    "\n",
    "def need_token(model_type: str, model_name_prefix: str=\"llama\"):\n",
    "    \"\"\"check if the model needs token\"\"\"\n",
    "    return model_type.startswith(model_name_prefix)\n",
    "\n",
    "def get_token(dir_setting: DirectorySetting):\n",
    "    \"\"\"get the token from the token file\"\"\"\n",
    "    token_file_path = dir_setting.get_token_file()\n",
    "    with open(token_file_path, \"r\") as file:\n",
    "        # file read add a new line to the token, remove it.\n",
    "        token = file.read().replace('\\n', '')\n",
    "    return token\n",
    "\n",
    "if need_token(model_type):\n",
    "    # kwargs = {\"use_auth_token\": get_token(dir_setting)}\n",
    "    token_kwargs = {\n",
    "        \"token\": get_token(dir_setting),\n",
    "        # \"truncation_side\": \"left\",\n",
    "        # \"return_tensors\": \"pt\",            \n",
    "                    }\n",
    "    print(\"huggingface token loaded\")\n",
    "else:\n",
    "    token_kwargs = {}\n",
    "    print(\"huggingface token is NOT needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model and then Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "\n",
    "pipeline_kwargs = {\n",
    "    \"torch_dtype\": torch.float16, #bfloat16 is not supported on MPS backend, float16 only on GPU accelerator\n",
    "    # torch_dtype=torch.float32,\n",
    "    # max_length=MAX_LENGTH,\n",
    "    \"max_length\" : None, # remove the total length of the generated response\n",
    "    \"max_new_tokens\" : 80,\n",
    "}    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e45de45e474a538a5fda9092611986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "# pretrained_model_name_or_path\n",
    "# TODO: use model config to set the max_length\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path=model_name,\n",
    "  device_map='auto',\n",
    "  # max_length= None, # remove the total length of the generated response\n",
    "  # max_new_tokens=80,\n",
    "  # quantization_config=bnb_config,\n",
    "  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "  # torch_dtype=torch.float16\n",
    "  **token_kwargs,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    # device='mps',\n",
    "    #max_position_embeddings=MAX_LENGTH,\n",
    "    #max_length=MAX_LENGTH,\n",
    "    # device_map=\"auto\", # put to GPU\n",
    "    device=\"cpu\", # put to CPU\n",
    "    # use_auth_token=token, #transformer==4.31.0\n",
    "    **token_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing token\n",
    "* https://huggingface.co/docs/tokenizers/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\"\"\"\n",
    "        Q: Roger has 3 tennis balls. \n",
    "        He buys 2 more cans of tennis balls. \n",
    "        Each can has 4 tennis balls. How many tennis balls does he have now?\\n\n",
    "        A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls.\n",
    "        3 + 8 = 11. The answer is 11.\\n\n",
    "        Q: The cafeteria had 23 apples. \n",
    "        If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n\n",
    "        \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "[1, 28705, 13, 5390, 1186, 28747, 14115, 659, 28705, 28770, 19552, 16852, 28723, 28705, 13, 5390, 650, 957, 846, 28705, 28750, 680, 277, 509, 302, 19552, 16852, 28723, 28705, 13, 5390, 7066, 541, 659, 28705, 28781, 19552, 16852, 28723, 1602, 1287, 19552, 16852, 1235, 400, 506, 1055, 28804, 13, 13, 5390, 330, 28747, 14115, 2774, 395, 28705, 28770, 16852, 28723, 28705, 28750, 277, 509, 302, 28705, 28781, 19552, 16852, 1430, 349, 28705, 28783, 19552, 16852, 28723, 13, 273, 28770, 648, 28705, 28783, 327, 28705, 28740, 28740, 28723, 415, 4372, 349, 28705, 28740, 28740, 28723, 13, 13, 5390, 1186, 28747, 415, 18302, 1623, 515, 553, 28705, 28750, 28770, 979, 2815, 28723, 28705, 13, 5390, 1047, 590, 1307, 28705, 28750, 28734, 298, 1038, 9957, 304, 7620, 28705, 28784, 680, 28725, 910, 1287, 979, 2815, 511, 590, 506, 28804, 13, 13, 273]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(f\"{len(input_test_encoded)}\")\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "        Q: Roger has 3 tennis balls. \n",
      "        He buys 2 more cans of tennis balls. \n",
      "        Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "\n",
      "        A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls.\n",
      "        3 + 8 = 11. The answer is 11.\n",
      "\n",
      "        Q: The cafeteria had 23 apples. \n",
      "        If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization_enabled = True\n",
    "# bitsandbytes quantization does not work with MPS \n",
    "\n",
    "# quantization_enabled = False\n",
    "\n",
    "# if quantization_enabled:\n",
    "#     compression_kwargs = {\n",
    "#         \"load_in_8bit\": True,\n",
    "#         # \"load_in_4bit\": True,\n",
    "#     }\n",
    "# else:\n",
    "#     compression_kwargs = {}\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, # optional\n",
    "    torch_dtype=torch.float16, #bfloat16 is not supported on MPS backend\n",
    "    # torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    # max_length=MAX_LENGTH,\n",
    "    max_length=None, # remove the total length of the generated response\n",
    "    max_new_tokens=80, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "    **token_kwargs,\n",
    "    # **compression_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install autopep8 or black extension in VSCode\n",
    "`shift + opt + F` to auto format python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Allocated memory : 28.008636 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from util.accelerator_utils import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.13'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic, time\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mistral_instruct_message(user_msg: str) -> str:\n",
    "    # https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "    mistral_system_message = f\"\"\"<s>[INST] You are a helpful, respectful and honest assistant.\n",
    "    Always answer as helpfully as possible using the context text provided.\n",
    "    Your answers should only answer the question once and not have any text after the answer is done.\\n\n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "    If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
    "    \\n\n",
    "    {user_msg}\\n\n",
    "    [/INST]\"\"\"\n",
    "    return mistral_system_message\n",
    "    \n",
    "def llama_instruct_message(user_msg: str) -> str:\n",
    "    # https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "    llama_system_message = f\"\"\"[INST]<<SYS>>You are a helpful, respectful and honest assistant.\n",
    "    Always answer as helpfully as possible using the context text provided.\n",
    "    Your answers should only answer the question once and not have any text after the answer is done.\\n\n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "    If you don't know the answer to a question, please don't share false information.<</SYS>>\n",
    "    \\n\n",
    "    {user_msg}\\n\n",
    "    [/INST]\"\"\"\n",
    "    return llama_system_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "<s>[INST] You are a helpful, respectful and honest assistant.\n",
      "    Always answer as helpfully as possible using the context text provided.\n",
      "    Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "    If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
      "    \n",
      "\n",
      "    Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "\n",
      "    [/INST]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# testing prompt\n",
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']\n",
    "\n",
    "# global variable model_name\n",
    "# print(model_name)\n",
    "\n",
    "def get_inputs(idx, model_name):\n",
    "    print(model_name)\n",
    "    if model_name.startswith(\"mi\"):\n",
    "        prompt = mistral_instruct_message(inputs[idx])\n",
    "    elif model_name.startedwith(\"ll\"):\n",
    "        prompt = llama_instruct_message(inputs[idx])\n",
    "    else:\n",
    "        prompt = inputs[idx]\n",
    "    return prompt\n",
    "\n",
    "# def get_inputs(idx):   \n",
    "#     return f\"{inputs[idx]}\" \n",
    "\n",
    "print(get_inputs(0, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: The cafeteria started with 23 apples. They used 20 for lunch, so that leaves 3 apples remaining. Then they bought 6 more apples, making the total number of apples 9. However, this answer is incorrect because it doesn't match the given information. The correct answer should be 29 (23 original apples\n",
      "--------------------\n",
      "walltime: 13.717299222946167 in secs.\n",
      "--------------------\n",
      "Allocated memory : 32.028671 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.001, max_new_tokens = 80, verbose=verbose)\n",
    "# batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_mps_memory(tokenizer, generator):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_MEMORY = False\n",
    "# CLEAR_MEMORY = True\n",
    "\n",
    "if CLEAR_MEMORY:\n",
    "    clear_mps_memory(tokenizer=tokenizer, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Allocated memory : 32.028671 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3.10",
   "language": "python",
   "name": "llm3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
