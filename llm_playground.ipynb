{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c70a197-b4a8-41af-9030-706699ef557e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608244d5-b5e5-4740-b44e-81ea0d67ca08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: huggingface_hub==0.20.2 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 1)) (0.20.2)\n",
      "Requirement already satisfied: transformers==4.36.2 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 4)) (4.36.2)\n",
      "Requirement already satisfied: urllib3==1.26.16 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 11)) (1.26.16)\n",
      "Requirement already satisfied: jsonschema==4.19.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 12)) (4.19.0)\n",
      "Requirement already satisfied: fastai==2.7.13 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 13)) (2.7.13)\n",
      "Requirement already satisfied: ipywidgets==8.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 15)) (8.1.0)\n",
      "Requirement already satisfied: click==8.1.7 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 17)) (8.1.7)\n",
      "Requirement already satisfied: multipledispatch==1.0.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 29)) (1.0.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.99 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 31)) (0.1.99)\n",
      "Requirement already satisfied: sacremoses==0.0.53 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 34)) (0.0.53)\n",
      "Requirement already satisfied: pypdf==3.15.5 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 36)) (3.15.5)\n",
      "Requirement already satisfied: accelerate==0.26.1 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 43)) (0.26.1)\n",
      "Collecting bitsandbytes==0.42.0 (from -r ./requirements.txt (line 45))\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: torch==2.1.2+cu118 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 49)) (2.1.2+cu118)\n",
      "Requirement already satisfied: torchaudio==2.1.2+cu118 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 50)) (2.1.2+cu118)\n",
      "Requirement already satisfied: torchvision==0.16.2+cu118 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 51)) (0.16.2+cu118)\n",
      "Requirement already satisfied: xformers==0.0.23.post1+cu118 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 52)) (0.0.23.post1+cu118)\n",
      "Requirement already satisfied: langchain==0.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 59)) (0.1.0)\n",
      "Requirement already satisfied: pydantic==1.10.13 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 64)) (1.10.13)\n",
      "Requirement already satisfied: unstructured==0.11.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 67)) (0.11.0)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 68)) (2.2.2)\n",
      "Requirement already satisfied: docarray==0.39.1 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 69)) (0.39.1)\n",
      "Requirement already satisfied: boto3==1.34.14 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 75)) (1.34.14)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (3.12.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (2023.9.0)\n",
      "Requirement already satisfied: requests in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.36.2->-r ./requirements.txt (line 4)) (1.22.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.36.2->-r ./requirements.txt (line 4)) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.36.2->-r ./requirements.txt (line 4)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.36.2->-r ./requirements.txt (line 4)) (0.3.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 12)) (22.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 12)) (5.12.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 12)) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 12)) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 12)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 12)) (0.10.2)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (23.3.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /home/jovyan/.local/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /home/jovyan/.local/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (1.5.29)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (3.4.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (1.2.4)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (1.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (9.4.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (0.24.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (1.7.0)\n",
      "Requirement already satisfied: spacy<4 in /home/jovyan/.local/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 13)) (3.7.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (8.11.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (3.0.8)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses==0.0.53->-r ./requirements.txt (line 34)) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses==0.0.53->-r ./requirements.txt (line 34)) (1.2.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate==0.26.1->-r ./requirements.txt (line 43)) (5.9.4)\n",
      "Requirement already satisfied: sympy in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.1.2+cu118->-r ./requirements.txt (line 49)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch==2.1.2+cu118->-r ./requirements.txt (line 49)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch==2.1.2+cu118->-r ./requirements.txt (line 49)) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.1.2+cu118->-r ./requirements.txt (line 49)) (2.1.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (2.0.24)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (0.6.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (0.0.10)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (0.1.8)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (0.0.77)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 59)) (8.2.3)\n",
      "Requirement already satisfied: chardet in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (5.2.0)\n",
      "Requirement already satisfied: filetype in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (0.4.27)\n",
      "Requirement already satisfied: lxml in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (4.9.3)\n",
      "Requirement already satisfied: nltk in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (4.12.2)\n",
      "Requirement already satisfied: emoji in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (2.8.0)\n",
      "Requirement already satisfied: python-iso639 in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (2023.6.15)\n",
      "Requirement already satisfied: langdetect in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (3.5.2)\n",
      "Requirement already satisfied: backoff in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (2.2.1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 67)) (1.15.0)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /home/jovyan/.local/lib/python3.8/site-packages (from docarray==0.39.1->-r ./requirements.txt (line 69)) (3.9.10)\n",
      "Requirement already satisfied: rich>=13.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from docarray==0.39.1->-r ./requirements.txt (line 69)) (13.7.0)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /home/jovyan/.local/lib/python3.8/site-packages (from docarray==0.39.1->-r ./requirements.txt (line 69)) (2.31.0.6)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/jovyan/.local/lib/python3.8/site-packages (from docarray==0.39.1->-r ./requirements.txt (line 69)) (0.9.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.14 in /home/jovyan/.local/lib/python3.8/site-packages (from boto3==1.34.14->-r ./requirements.txt (line 75)) (1.34.14)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/jovyan/.local/lib/python3.8/site-packages (from boto3==1.34.14->-r ./requirements.txt (line 75)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/jovyan/.local/lib/python3.8/site-packages (from boto3==1.34.14->-r ./requirements.txt (line 75)) (0.10.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jovyan/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r ./requirements.txt (line 59)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r ./requirements.txt (line 59)) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jovyan/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r ./requirements.txt (line 59)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jovyan/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r ./requirements.txt (line 59)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.35.0,>=1.34.14->boto3==1.34.14->-r ./requirements.txt (line 75)) (2.8.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jovyan/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0->-r ./requirements.txt (line 59)) (3.20.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema==4.19.0->-r ./requirements.txt (line 12)) (3.15.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (2.14.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (4.8.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.0->-r ./requirements.txt (line 59)) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.8/site-packages (from langchain-core<0.2,>=0.1.7->langchain==0.1.0->-r ./requirements.txt (line 59)) (3.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jovyan/.local/lib/python3.8/site-packages (from rich>=13.1.0->docarray==0.39.1->-r ./requirements.txt (line 69)) (3.0.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (2.0.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (6.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (67.6.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (3.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/jovyan/.local/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.0->-r ./requirements.txt (line 59)) (3.0.1)\n",
      "Requirement already satisfied: types-urllib3 in /home/jovyan/.local/lib/python3.8/site-packages (from types-requests>=2.28.11.6->docarray==0.39.1->-r ./requirements.txt (line 69)) (1.26.25.14)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jovyan/.local/lib/python3.8/site-packages (from typing-inspect>=0.8.0->docarray==0.39.1->-r ./requirements.txt (line 69)) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->unstructured==0.11.0->-r ./requirements.txt (line 67)) (2.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch==2.1.2+cu118->-r ./requirements.txt (line 49)) (2.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.13->-r ./requirements.txt (line 13)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.13->-r ./requirements.txt (line 13)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.13->-r ./requirements.txt (line 13)) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->fastai==2.7.13->-r ./requirements.txt (line 13)) (2023.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->fastai==2.7.13->-r ./requirements.txt (line 13)) (3.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jovyan/.local/lib/python3.8/site-packages (from sympy->torch==2.1.2+cu118->-r ./requirements.txt (line 49)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.8/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain==0.1.0->-r ./requirements.txt (line 59)) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray==0.39.1->-r ./requirements.txt (line 69)) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.2.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/jovyan/.local/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai==2.7.13->-r ./requirements.txt (line 13)) (0.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 15)) (0.2.2)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --trusted-host https://download.pytorch.org/whl/cu118 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48662e9c-04da-455f-a80d-bd8136ee5f2e",
   "metadata": {},
   "source": [
    "#### Useful installation for KF notebook 1.7.0 cu111 drivers\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade transformers==4.31.0\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==1.10.2+cu111 fastai==2.7.12 fastcore==1.5.29 fastdownload==0.0.7 torchvision==0.11.3+cu111 --extra-index-url https://download.pytorch.org/whl/cu111\n",
    "#!{sys.executable} -m pip install --user --upgrade accelerate==0.20.3\n",
    "```\n",
    "\n",
    "We shall use the cuda 11.8 version (Cuda118)\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==2.0.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "`xformers==0.0.21` need `torch==2.0.1``\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade xformers==0.0.21 torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "```\n",
    "\n",
    "show js loading with ipywidgets\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0 comm==0.1.4 jupyterlab-widgets==3.0.8 widgetsnbextension==4.0.8\n",
    "```\n",
    "\n",
    "uninstall\n",
    "```shell\n",
    "#!{sys.executable} -m pip uninstall accelerator transformers xformers torch -y \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5748b3-454f-4a70-8f1f-bef58eb85506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/llm-models/core-kind/yinwang/models'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.accelerator_utils import AcceleratorHelper, DIR_MODE_MAP, DirectorySetting\n",
    "import os\n",
    "\n",
    "dir_setting = dir_setting=DIR_MODE_MAP.get(\"kf_notebook\")\n",
    "gpu_helper = AcceleratorHelper()\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)\n",
    "gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "\n",
    "os.environ['XDG_CACHE_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-9579f618-ddae-5958-9285-3207382f0b36\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from platform import python_version\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\" # for debugging\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e320b2a-cac5-421a-abd5-036c6212b612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mistral8x7B-01\":   \"mistralai/Mistral-Mixtral-8x7B-v0.1\", \n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4ad7f1-d083-48a3-9415-1cb6535af553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n",
      "2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76144154-fb66-4792-9da5-edd5a1993c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "model_type = \"llama13B-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "def need_token(model_type: str, model_name_prefix: str=\"llama\"):\n",
    "    \"\"\"check if the model needs token\"\"\"\n",
    "    return model_type.startswith(model_name_prefix)\n",
    "\n",
    "def get_token(dir_setting: DirectorySetting):\n",
    "    \"\"\"get the token from the token file\"\"\"\n",
    "    token_file_path = dir_setting.get_token_file()\n",
    "    with open(token_file_path, \"r\") as file:\n",
    "        # file read add a new line to the token, remove it.\n",
    "        token = file.read().replace('\\n', '')\n",
    "    return token\n",
    "\n",
    "if need_token(model_type):\n",
    "    # kwargs = {\"use_auth_token\": get_token(dir_setting)}\n",
    "    token_kwargs = {\n",
    "        \"token\": get_token(dir_setting),\n",
    "        # \"truncation_side\": \"left\",\n",
    "        # \"return_tensors\": \"pt\",            \n",
    "                    }\n",
    "    print(\"huggingface token loaded\")\n",
    "else:\n",
    "    token_kwargs = {}\n",
    "    print(\"huggingface token is NOT needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-13b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    device=\"cpu\",\n",
    "    trust_remote_code=True,\n",
    "    **token_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a18b436d-e07e-4c3b-a1f7-e73c602b7638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04e5a080-7da4-40a3-bd63-ca14f39df606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2375892025.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    }\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "quantization_enabled = True\n",
    "# bitsandbytes quantization does not work with MPS \n",
    "# quantization_enabled = False\n",
    "\n",
    "if quantization_enabled:\n",
    "    compression_kwargs = {\n",
    "        \"load_in_8bit\": True,\n",
    "        \"\"\n",
    "        # \"load_in_4bit\": True,\n",
    "    }\n",
    "else:\n",
    "    compression_kwargs = {}\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer, # optional\n",
    "    # torch_dtype=torch.float16, #bfloat16 is not supported on MPS backend\n",
    "    # torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    # max_length=MAX_LENGTH,\n",
    "    max_length=None, # remove the total length of the generated response\n",
    "    max_new_tokens=100, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "    **token_kwargs,\n",
    "    **compression_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1ac8ee4-f7ac-440e-a081-136fc122fa88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 9.041016 GB\n",
      "Allocated memory : 9.039349 GB\n",
      "Free      memory : 0.001667 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from util.accelerator_utils import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc0308f6-5431-4ed9-af26-b9d0be6c3d03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9fbc6de-0c5e-4db7-a74a-6ccf0f8646de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09491b84-0de7-45f7-b903-144c473e59d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant.\n",
      "Always answer as helpfully as possible using the context text provided.\n",
      "Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message=\"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "# testing prompt\n",
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']\n",
    "\n",
    "def get_inputs(idx):   \n",
    "    return f\"{system_message}{inputs[idx]}\"\n",
    "\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d860ac56-4ce7-4763-ad72-bfc4e86c5aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['load_in_8bit'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m batch_answers \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36mchat_gen.<locals>.local\u001b[0;34m(input_prompts, temperature, max_new_tokens, verbose)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mdo_sample, top_k, num_return_sequences, eos_token_id are the settings \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mthe TextGenerationPipeline\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mhttps://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 15\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pad_token_id=tokenizer.eos_token_id, # for mistral\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_length=200,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 200 # max number of tokens to generate in the output\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# without this output begins repeating\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# for seq in sequences:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     print(f\"Result: \\n{seq['generated_text']}\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m batch_result \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1530\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1530\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1344\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1347\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['load_in_8bit'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.001, max_new_tokens = 80, verbose=verbose)\n",
    "# batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_answer(answer: list)-> None:\n",
    "    if DEBUG:\n",
    "        print(\"-\"*10)\n",
    "        print(answer[0])\n",
    "        print(\"-\"*10)\n",
    "        print(answer[0].split(\"\\n\")[-1])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64be04-c528-426b-91f1-47b71996d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def free_memory_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def local():\n",
    "        l_generator = generator\n",
    "        l_tokenizer = tokenizer\n",
    "        #l_generator.cpu()\n",
    "        #l_tokenizer.cpu()\n",
    "        # model.cpu()\n",
    "        \n",
    "        del l_tokenizer, l_generator\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        #for device_idx in range(torch.cuda.device_count()):\n",
    "        #    print(device_idx)\n",
    "        #    device = torch.device(f\"cuda:{device_idx}\")\n",
    "        #    device.reset()\n",
    "    return local    \n",
    "\n",
    "free_memory = free_memory_gen(generator, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfad65b-f23b-4e99-b724-57dd530b99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of thoughts prompting\n",
    "\n",
    "# testing prompt\n",
    "input='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat(input, False)\n",
    "#print_answer(answer)\n",
    "print(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d410207-f7de-4ebf-80cc-436217ee8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_text_loader import PDFHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc455c76-8203-4541-a82e-80444bf1835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = PDFHelper(data_folder = \"./data/medreports\", file_pattern=\"KK-SCIVIAS-*.pdf\")\n",
    "#context = loader.read_pdf(1)\n",
    "\n",
    "#input = f\"Context: Patient: Fried\\nFrage: Welcher name hat der Patient?\\nAntwort: Name ist Fried\\nContext: {context}\\nFrage: Welcher name hat die Patientin?\\nAntwort: die Patientin hat name \"\n",
    "#print(input)\n",
    "#chat(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942d22b-8e04-4d37-9e08-fe3f25e5d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"core-kind/yinwang\"\n",
    "loader = PDFHelper(data_folder = f\"{DATA_ROOT}/{data_path}/data/medreports\", file_pattern=\"KK-SCIVIAS-*.txt\")\n",
    "context = loader.read_txt(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861587f9-f41e-4ce3-903b-ec50b17f4cce",
   "metadata": {},
   "source": [
    "#### zero shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5316fa-82d8-4f47-98fa-9506916a1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name\n",
    "input=f\"Can you tell me the name of the patient from the folowing doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6b804-67bd-4c54-a7c2-0e5ef14aa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input)\n",
    "# 6810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a0f12-7d99-4aba-9c8a-5fb1e28f456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c299015-c5b7-425b-8a79-ba155491c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age\n",
    "input=f\"Can you tell me the age of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf966a-76c1-4e7d-8a6d-0b356b2b86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d472f-80db-4d73-ac73-f673bc5fe279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnosis\n",
    "input=f\"Can you tell me the diagnosis of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7927d-9cba-4cbf-a05c-4fcaf886c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa11f-0eb2-4dac-aa70-194cfe8e577d",
   "metadata": {},
   "source": [
    "#### Chain-of-thoughts prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name prompt\n",
    "input = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {context}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881162a-6514-4f31-a64c-60718b2f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61eea4-3731-4ba5-8c1b-e392cc6fe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "input = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{context}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "# print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31ec3d-1912-4a07-aab0-c84151783112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "#len(input)\n",
    "# > 6913 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbeb59-1fe4-4ab0-b004-05b2fe2d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a401151-af8b-46f4-8ef2-0177ae075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose prompt\n",
    "input=f\"Context:\\nPatient: Fried is a 34-year-old patient, Diagnoses: Influenza (J09.X2) \\nQuestion:\\nWhat diagnoses has the patient? \\nAnswer:\\nFried is a patient, 34 year-old, has diagnoses Influenza (J09.X2). The answers is Influenza (J09.X2)\\nContext:\\n{context}\\nQuestion:\\nWhat diagnoses has the patient?\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd94444-a458-40dd-b95d-2be26f9ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b6e3c-3d58-4cf6-b834-1d066519e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d74a3-4ef3-4379-9075-a3944812244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4aca2a-e1cc-4769-bdd8-268906d978d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4464c-2597-4eed-b3b1-16e67d966fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
