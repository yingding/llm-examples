{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d49fc33-61c9-41aa-a446-c9cf79cafe2c",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "* https://www.youtube.com/watch?v=ypzmPwLH_Q4\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7ed45-fe39-42b0-ae08-80b6e7be97d8",
   "metadata": {},
   "source": [
    "# Install cuda toolkit\n",
    "* https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#conda-installation\n",
    "\n",
    "1. Run the following cmd in terminal\n",
    "```shell\n",
    "conda install -y cuda -c nvidia/label/cuda-11.8.0\n",
    "# conda install -y cuda-toolkit-11.8.0 -c nvidia/label/cuda-11.8.0\n",
    "conda install -y anaconda::make\n",
    "# g++\n",
    "conda install -y gxx -c conda-forge\n",
    "```\n",
    "\n",
    "2. compile you bitsandbytes:\n",
    "```shell\n",
    "git clone https://github.com/timdettmers/bitsandbytes.git\n",
    "cd bitsandbytes\n",
    "CUDA_VERSION=118 make cuda11x\n",
    "# CUDA_VERSION=118 make cuda118\n",
    "python setup.py install\n",
    "# python setup.py install --user # doesn't work with --user flag\n",
    "```\n",
    "\n",
    "Debug test bitsandbytes:\n",
    "```shell\n",
    "python -m bitsandbytes\n",
    "```\n",
    "\n",
    "Reference:\n",
    "* https://stackoverflow.com/questions/4495120/combine-user-with-prefix-error-with-setup-py-install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e47ee59-c1c0-4119-bbdc-4c7fb6a5bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y cuda -c nvidia/label/cuda-11.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a185bc88-6df2-489d-8cf0-207563d140d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0787cfd1-8c34-4255-9025-87bf64d59345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                       1.8.22\n",
      "kfp-pipeline-spec         0.1.16\n",
      "kfp-server-api            1.8.5\n"
     ]
    }
   ],
   "source": [
    "# examing the kfp python sdk version inside a KubeFlow v1.5.1\n",
    "!{sys.executable} -m pip list | grep kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3b4487-c06c-47b2-9867-29990621e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46791c4f-e00e-4793-bfab-abe7b9344576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bfc1f14-17f8-4307-aa90-aa5ed336c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_enabled = True\n",
    "# bitsandbytes quantization does not work with MPS \n",
    "# quantization_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314f8910-4cb1-4e17-97eb-c34e731b2039",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda-11.8/targets/x86_64-linux/lib/:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# find / -name libcuda.so 2>/dev/null\n",
    "# os.environ['LD_LIBRARY_PATH']='/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib/x86_64-linux-gnu'\n",
    "# os.environ['LD_LIBRARY_PATH']='/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib/:/opt/conda/pkgs/cuda-cudart-dev-11.8.89-0/lib/'\n",
    "\n",
    "# use `find / -name libcudart.so 2>/dev/null`\n",
    "# to get the path /usr/local/cuda-11.8/targets/x86_64-linux/lib/\n",
    "lb_library_path = os.environ['LD_LIBRARY_PATH']\n",
    "lb_library_path = f\"/usr/local/cuda-11.8/targets/x86_64-linux/lib/:{lb_library_path}\"\n",
    "print(lb_library_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "536793f8-5bb4-4dac-8ca6-ec8e0872f24a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bitsandbytes.cextension import CUDASetup\n",
    "# import torch\n",
    "# lib = CUDASetup.get_instance().lib\n",
    "# lib.cadam32bit_g32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c70a197-b4a8-41af-9030-706699ef557e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "608244d5-b5e5-4740-b44e-81ea0d67ca08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --trusted-host https://download.pytorch.org/whl/cu118 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48662e9c-04da-455f-a80d-bd8136ee5f2e",
   "metadata": {},
   "source": [
    "#### Useful installation for KF notebook 1.7.0 cu111 drivers\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade transformers==4.31.0\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==1.10.2+cu111 fastai==2.7.12 fastcore==1.5.29 fastdownload==0.0.7 torchvision==0.11.3+cu111 --extra-index-url https://download.pytorch.org/whl/cu111\n",
    "#!{sys.executable} -m pip install --user --upgrade accelerate==0.20.3\n",
    "```\n",
    "\n",
    "We shall use the cuda 11.8 version (Cuda118)\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==2.0.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "`xformers==0.0.21` need `torch==2.0.1``\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade xformers==0.0.21 torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "```\n",
    "\n",
    "show js loading with ipywidgets\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0 comm==0.1.4 jupyterlab-widgets==3.0.8 widgetsnbextension==4.0.8\n",
    "```\n",
    "\n",
    "uninstall\n",
    "```shell\n",
    "#!{sys.executable} -m pip uninstall accelerator transformers xformers torch -y \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f5748b3-454f-4a70-8f1f-bef58eb85506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/llm-models/core-kind/yinwang/models'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.accelerator_utils import (\n",
    "    AcceleratorHelper, \n",
    "    DIR_MODE_MAP, \n",
    "    DirectorySetting,\n",
    "    TokenHelper as th\n",
    ")\n",
    "import os\n",
    "\n",
    "dir_setting = dir_setting=DIR_MODE_MAP.get(\"kf_notebook\")\n",
    "gpu_helper = AcceleratorHelper()\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)\n",
    "gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "\n",
    "os.environ['XDG_CACHE_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-6133b50c-6a86-5721-bcd6-932fa6798089\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from platform import python_version\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\" # for debugging\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e320b2a-cac5-421a-abd5-036c6212b612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f4ad7f1-d083-48a3-9415-1cb6535af553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76144154-fb66-4792-9da5-edd5a1993c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "model_type = \"mixtral8x7B-01\"\n",
    "# model_type = \"mixtral8x7B-inst01\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "# model_type = \"llama70B-chat\" # 37GB GPU MEM is too small for 4bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "def gen_token_kwargs():\n",
    "    # call method from token helper class\n",
    "    if th.need_token(model_type):\n",
    "        # kwargs = {\"use_auth_token\": get_token(dir_setting)}\n",
    "        token_kwargs = {\n",
    "            \"token\": th.get_token(dir_setting),\n",
    "            # \"truncation_side\": \"left\",\n",
    "            # \"return_tensors\": \"pt\",            \n",
    "                        }\n",
    "        print(\"huggingface token loaded\")\n",
    "    else:\n",
    "        token_kwargs = {}\n",
    "        print(\"huggingface token is NOT needed\")\n",
    "    return token_kwargs\n",
    "\n",
    "token_kwargs = gen_token_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77ceca5c-2926-45cc-8949-8d3515bae37a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 1g.10gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 14\n",
      "Physical  memory : 9.500000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from util.accelerator_utils import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9f3db6a-64d4-4122-8418-44ee95681ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7GB'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49cbb3-10e7-4c90-9e94-28e0ac3925f6",
   "metadata": {},
   "source": [
    "## Following this approach to load llama2 model with bitsandbytes\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85613cf3-925a-47f6-9fe7-002a2732f28b",
   "metadata": {},
   "source": [
    "## 4bit quantization\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 7.085938 GB\n",
    "Allocated memory : 6.809501 GB\n",
    "Free      memory : 0.276437 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "855f6076-3849-465a-9c28-4ecd96e8bc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29406956-7d27-497f-b23b-00ee5c5f1e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mixtral'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n\u001b[1;32m      2\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mBitsAndBytesConfig(\n\u001b[1;32m      3\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mbfloat16\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#'decapoda-research/llama-7b-hf',\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtoken_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:1064\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m-> 1064\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:761\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    762\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    763\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mixtral'"
     ]
    }
   ],
   "source": [
    "from torch import bfloat16\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name, #'decapoda-research/llama-7b-hf',\n",
    "  device_map='auto',\n",
    "  quantization_config=bnb_config,\n",
    "  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "  **token_kwargs,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e6909-3578-4700-b39e-a650e17a43ce",
   "metadata": {},
   "source": [
    "## 8bit quantization\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 13.185547 GB\n",
    "Allocated memory : 12.572203 GB\n",
    "Free      memory : 0.613344 GB\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "\n",
    "n.a\n",
    "\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0f61c44-f2b1-4dfc-bf5c-43d2834b4797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#  model_name, #'decapoda-research/llama-7b-hf',\n",
    "#  device_map='auto',\n",
    "#  load_in_8bit=True,\n",
    "#  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "#  **token_kwargs,  \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31ff607c-54a5-43da-ad33-d13bc458f5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#  model_name, #'decapoda-research/llama-7b-hf',\n",
    "#  device_map='auto',\n",
    "#  load_in_8bit=True,\n",
    "#  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "#  **token_kwargs,  \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a56ce2c-92a4-4199-8da9-a0c4fcc911bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.496094 GB\n",
      "Allocated memory : 23.303491 GB\n",
      "Free      memory : 0.192603 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "050e9165-fb53-4595-80ca-a6255d098a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a63aa23c-4383-4bd4-92ad-fdf33cfce29d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixtralForCausalLM(\n",
      "  (model): MixtralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MixtralDecoderLayer(\n",
      "        (self_attn): MixtralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MixtralRotaryEmbedding()\n",
      "        )\n",
      "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
      "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): MixtralRMSNorm()\n",
      "        (post_attention_layernorm): MixtralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MixtralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.eval())\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8ea5b-54b5-421e-95d0-58d88ec77a0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Issue with \n",
    "probability tensor contains either inf, nan or element < 0\n",
    "* https://github.com/facebookresearch/llama/issues/380#issuecomment-1716832417\n",
    "* https://github.com/facebookresearch/llama/issues/380\n",
    "\n",
    "The bitsandbytes quantized model might be corrupt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4d44570-fde0-4ca2-91a8-597206c7a0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    **token_kwargs,\n",
    ")\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1ef10d2-a1c8-4513-9010-292504a159f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=80,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68b515e6-3377-4bca-bc36-b087db5aa85a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain to me the difference between nuclear fission and fusion.\n",
      "\n",
      "A: Fusion is when two atoms are combined together, like hydrogen into helium. This process releases a lot of energy because it takes less energy to combine them than they have in total. In contrast, fission is when an atom splits apart, such as uranium or plutonium. The splitting of these atoms also release a large amount of energy.\n"
     ]
    }
   ],
   "source": [
    "res = generator(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5da7da7-5c3e-4d52-8713-353416007e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.812500 GB\n",
      "Allocated memory : 23.311425 GB\n",
      "Free      memory : 0.501075 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90052e36-c96f-4e2f-b03c-61f98f6a8e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_cuda_memory(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        # tokenizer is load in cpu\n",
    "        # tokenizer.model.cpu()\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n",
    "    \n",
    "# clear_cuda_memory(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac18541-1aef-4885-84b1-216c15cdbe1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HuggingFace Pipeline doesn't seem to work with Bits and Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6315ce5c-b9d8-4661-8a87-6c15df9913cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b81f4d72-9725-4daa-86f7-602b534d9b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "#bnb_config = transformers.BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type='nf4',\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype=bfloat16\n",
    "#)\n",
    "\n",
    "#bnb_config = transformers.BitsAndBytesConfig(\n",
    "#    load_in_8bit=True,\n",
    "#    bnb_4bit_quant_type='nf4',\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype=bfloat16\n",
    "#)\n",
    "\n",
    "#model_config = transformers.AutoConfig.from_pretrained(\n",
    "#    model_name,\n",
    "#    **token_kwargs,\n",
    "#)\n",
    "\n",
    "#print(bnb_config)\n",
    "#print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9dd2513-5d73-446a-9ccb-471963c668c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#    model_name,\n",
    "#    trust_remote_code=True,\n",
    "#    config=model_config,\n",
    "#    quantization_config=bnb_config,\n",
    "#    device_map='auto',\n",
    "#    **token_kwargs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fe4aac8-fa61-4026-a821-b546f45656e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bitsandbytes.cextension import CUDASetup\n",
    "# import torch\n",
    "# lib = CUDASetup.get_instance().lib\n",
    "# lib.cadam32bit_g32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\n",
    "#    model_name, \n",
    "#    device=\"cpu\",\n",
    "#    trust_remote_code=True,\n",
    "#    **token_kwargs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a18b436d-e07e-4c3b-a1f7-e73c602b7638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04e5a080-7da4-40a3-bd63-ca14f39df606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 3 µs, total: 13 µs\n",
      "Wall time: 25.7 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# quantization_enabled = True\n",
    "# bitsandbytes quantization does not work with MPS \n",
    "# quantization_enabled = False\n",
    "\n",
    "#if quantization_enabled:\n",
    "#    compression_kwargs = {\n",
    "#        \"load_in_8bit\": True,\n",
    "#        # \"load_in_4bit\": True,\n",
    "#    }\n",
    "#else:\n",
    "#    compression_kwargs = {\n",
    "#        \"torch_dtype\": torch.float16\n",
    "#    }\n",
    "\n",
    "#generator = transformers.pipeline(\n",
    "#    \"text-generation\",\n",
    "#    model=model_name,\n",
    "#    tokenizer=tokenizer, # optional\n",
    "#    # torch_dtype=torch.float16, #bfloat16 is not supported on MPS backend\n",
    "#    # torch_dtype=torch.float32,\n",
    "#    device_map=\"auto\",\n",
    "#    # max_length=MAX_LENGTH,\n",
    "#    max_length=None, # remove the total length of the generated response\n",
    "#    max_new_tokens=100, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "#    use_fast = True,\n",
    "#    **token_kwargs,\n",
    "#    **compression_kwargs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1ac8ee4-f7ac-440e-a081-136fc122fa88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from util.accelerator_utils import AcceleratorStatus\n",
    "\n",
    "#gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "#gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc0308f6-5431-4ed9-af26-b9d0be6c3d03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
       "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9fbc6de-0c5e-4db7-a74a-6ccf0f8646de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09491b84-0de7-45f7-b903-144c473e59d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant.\n",
      "Always answer as helpfully as possible using the context text provided.\n",
      "Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message=\"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "# testing prompt\n",
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']\n",
    "\n",
    "def get_inputs(idx):   \n",
    "    return f\"{system_message}{inputs[idx]}\"\n",
    "\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d860ac56-4ce7-4763-ad72-bfc4e86c5aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: Started with 23 - 20 = 3. Bought 6 more so the total number of apples was 9.\n",
      "Q: There are 57 dogs signed up for the dog show. At the beginning of the show, there were 49 present. 3 left early and one arrived late. How many dogs were at the show?\n",
      "\n",
      "--------------------\n",
      "walltime: 10.02276349067688 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.863281 GB\n",
      "Allocated memory : 23.311425 GB\n",
      "Free      memory : 0.551856 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.001, max_new_tokens = 80, verbose=verbose)\n",
    "# batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_answer(answer: list)-> None:\n",
    "    if DEBUG:\n",
    "        print(\"-\"*10)\n",
    "        print(answer[0])\n",
    "        print(\"-\"*10)\n",
    "        print(answer[0].split(\"\\n\")[-1])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d49de0f-2ff0-49ee-a4f0-456ae41f035d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 0.781250 GB\n",
      "Allocated memory : 0.695191 GB\n",
      "Free      memory : 0.086059 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "def clear_cuda_memory(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        # tokenizer is load in cpu\n",
    "        # tokenizer.model.cpu()\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n",
    "    \n",
    "clear_cuda_memory(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d64be04-c528-426b-91f1-47b71996d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gc\n",
    "#def free_memory_gen(\n",
    "#    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "#    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):\n",
    "#    \"\"\"\n",
    "#    \"\"\"\n",
    "#    def local():\n",
    "#        l_generator = generator\n",
    "#        l_tokenizer = tokenizer\n",
    "#        #l_generator.cpu()\n",
    "#        #l_tokenizer.cpu()\n",
    "#        # model.cpu()\n",
    "#        \n",
    "#        del l_tokenizer, l_generator\n",
    "#        gc.collect()\n",
    "#        torch.cuda.empty_cache()\n",
    "#        #for device_idx in range(torch.cuda.device_count()):\n",
    "#        #    print(device_idx)\n",
    "#        #    device = torch.device(f\"cuda:{device_idx}\")\n",
    "#        #    device.reset()\n",
    "#    return local\n",
    "#\n",
    "# free_memory = free_memory_gen(generator, tokenizer)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
