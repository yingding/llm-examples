{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84683eed-edfb-4c7a-94dc-9b845a585985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade kfp==2.0.0b13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae1027a-6b43-4efa-a5af-582448b845dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the ipywidgets and restart kernel the javascript widget for Huggingface download widget will show.\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub==0.16.4\n",
      "transformers==4.32.1\n",
      "torch==2.0.1\n",
      "accelerate==0.22.0\n",
      "# huggingface_hub use_auth_token need this.\n",
      "urllib3==2.0.4 \n",
      "jsonschema==4.19.0\n",
      "fastai==2.7.12\n",
      "torchaudio==2.0.2\n",
      "torchvision==0.15.2\n",
      "# for showing download widget in jupyter notebook\n",
      "ipywidgets==8.1.0\n",
      "# for python script input arg generation\n",
      "click==8.1.7\n",
      "# argparse==1.4.0\n",
      "#\n",
      "# monitor nvidia gpu usage\n",
      "# have no permission to access\n",
      "# gpustat==1.1.1\n",
      "# nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv\n",
      "# from a kubeflow notebook\n",
      "#\n",
      "# kfp==1.8.22\n",
      "# \n",
      "# python method overload\n",
      "multipledispatch==1.0.0"
     ]
    }
   ],
   "source": [
    "!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbafbba5-b1fc-429f-b792-e829e0e5ba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub==0.16.4 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 1)) (0.16.4)\n",
      "Requirement already satisfied: transformers==4.32.1 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 2)) (4.32.1)\n",
      "Requirement already satisfied: torch==2.0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: accelerate==0.22.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 4)) (0.22.0)\n",
      "Requirement already satisfied: urllib3==2.0.4 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 6)) (2.0.4)\n",
      "Requirement already satisfied: jsonschema==4.19.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 7)) (4.19.0)\n",
      "Requirement already satisfied: fastai==2.7.12 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 8)) (2.7.12)\n",
      "Requirement already satisfied: torchaudio==2.0.2 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 9)) (2.0.2)\n",
      "Requirement already satisfied: torchvision==0.15.2 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 10)) (0.15.2)\n",
      "Requirement already satisfied: ipywidgets==8.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 12)) (8.1.0)\n",
      "Requirement already satisfied: click==8.1.7 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 14)) (8.1.7)\n",
      "Requirement already satisfied: multipledispatch==1.0.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 26)) (1.0.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (2023.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: requests in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (3.12.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.32.1->-r ./requirements.txt (line 2)) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.32.1->-r ./requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.32.1->-r ./requirements.txt (line 2)) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.32.1->-r ./requirements.txt (line 2)) (1.22.4)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (11.10.3.66)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: sympy in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.0.1->-r ./requirements.txt (line 3)) (11.7.101)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate==0.22.0->-r ./requirements.txt (line 4)) (5.9.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 7)) (22.2.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 7)) (0.10.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 7)) (5.12.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 7)) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 7)) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 7)) (0.30.2)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (1.0.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (3.4.2)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (23.0.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (0.24.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /home/jovyan/.local/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (1.5.29)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /home/jovyan/.local/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (0.0.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (1.2.4)\n",
      "Requirement already satisfied: pillow>6.0.0 in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (9.4.0)\n",
      "Requirement already satisfied: spacy<4 in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.12->-r ./requirements.txt (line 8)) (3.5.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (8.11.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (3.0.8)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (4.0.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.1.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (5.9.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r ./requirements.txt (line 3)) (67.6.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r ./requirements.txt (line 3)) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/jovyan/.local/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1->-r ./requirements.txt (line 3)) (3.27.2)\n",
      "Requirement already satisfied: lit in /home/jovyan/.local/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1->-r ./requirements.txt (line 3)) (16.0.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema==4.19.0->-r ./requirements.txt (line 7)) (3.15.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (2.14.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.6.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.18.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (3.0.38)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (4.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (1.10.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (6.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (2.4.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (8.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.16.4->-r ./requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch==2.0.1->-r ./requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.12->-r ./requirements.txt (line 8)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.12->-r ./requirements.txt (line 8)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.12->-r ./requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.12->-r ./requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->fastai==2.7.12->-r ./requirements.txt (line 8)) (2023.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->fastai==2.7.12->-r ./requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->fastai==2.7.12->-r ./requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jovyan/.local/lib/python3.8/site-packages (from sympy->torch==2.0.1->-r ./requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->fastai==2.7.12->-r ./requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai==2.7.12->-r ./requirements.txt (line 8)) (0.0.4)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 12)) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea92d003-03af-4750-9a15-85f2eb2cffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-51f84540-9ebb-1d44-7bb7-3c62ae55c20e)\n",
      "  MIG 2g.20gb     Device  0: (UUID: MIG-0efc9f06-6dca-5886-98af-0273ca7fde51)\n"
     ]
    }
   ],
   "source": [
    "list=!nvidia-smi -L\n",
    "for i in range(len(list)):\n",
    "    print(list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76206acf-2f57-4460-b0cd-d94af56fa07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-0efc9f06-6dca-5886-98af-0273ca7fde51\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_device_uuid(input: str) -> str:\n",
    "    try:\n",
    "        # r'' before the search pattern indicates it is a raw string, \n",
    "        # otherwise \"\" instead of single quote\n",
    "        uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "    except AttributeError:\n",
    "        # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "        uuid = \"\"\n",
    "    return uuid    \n",
    "\n",
    "# skip the first GPU ID, only get the MIG IDs, using python list slice over index access\n",
    "uuid_list = [get_device_uuid(e) for e in list[1:]]\n",
    "# print(uuid_list)\n",
    "UUIDs = \",\".join(uuid_list)\n",
    "print(UUIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-0efc9f06-6dca-5886-98af-0273ca7fde51\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from platform import python_version\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = UUIDs # \"0,1,2\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e320b2a-cac5-421a-abd5-036c6212b612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(20748107776, 20937965568)\n",
      "19.32318115234375\n",
      "19.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90c0dfd5-9a5a-460a-a723-e62cad74536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/58216000/get-total-amount-of-free-gpu-memory-and-available-using-pytorch\n",
    "# torch.cuda.device_count()\n",
    "# t = torch.cuda.get_device_properties(0).total_memory\n",
    "# r = torch.cuda.memory_reserved(0)\n",
    "# a = torch.cuda.memory_allocated(0)\n",
    "# f = r-a  # free inside reserved\n",
    "# print(t/1024**3)\n",
    "# print(r/1024**3)\n",
    "# print(a/1024**3)\n",
    "# print(f/1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ff43674-5abd-4da1-88d5-8c7daa6aca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device_name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Multi_processor  : 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://stackoverflow.com/questions/58216000/get-total-amount-of-free-gpu-memory-and-available-using-pytorch\n",
    "# from typing import Tuple\n",
    "\n",
    "def byte_gb_info(byte_mem) -> str:\n",
    "    \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "    # format the f string float with :.2f to decimal digits\n",
    "    # https://zetcode.com/python/fstring/\n",
    "    return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "def accelerator_mem_info(device_idx: int):\n",
    "    # total\n",
    "    t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "    # usable\n",
    "    r = torch.cuda.memory_reserved(device_idx)\n",
    "    # allocated\n",
    "    a = torch.cuda.memory_allocated(device_idx)\n",
    "    # still free\n",
    "    f = r-a\n",
    "    # unit = \"GB\"   \n",
    "    print( # \"GPU memory info:\\n\" + \n",
    "          f\"Physical  memory : {byte_gb_info(t)}\\n\" + \n",
    "          f\"Reserved  memory : {byte_gb_info(r)}\\n\" + \n",
    "          f\"Allocated memory : {byte_gb_info(a)}\\n\" + \n",
    "          f\"Free      memory : {byte_gb_info(f)}\")\n",
    "\n",
    "    \n",
    "def accelerator_compute_info(device_idx: int):\n",
    "    name = torch.cuda.get_device_properties(device_idx).name\n",
    "    count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "    print(f\"Device_name      : {name} \\n\" +\n",
    "          f\"Multi_processor  : {count}\")    \n",
    "\n",
    "    \n",
    "def gpu_usage():        \n",
    "    num_of_gpus = torch.cuda.device_count();\n",
    "    # this shows only the gpu device, not the MIG\n",
    "    print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "    # available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "    # available_gpus = [torch.cuda.get_device_properties(i).name for i in range(torch.cuda.device_count())]\n",
    "    # print(f\"device_mig_info: {available_gpus}\")\n",
    "    for device_idx in range(torch.cuda.device_count()):\n",
    "        print(\"-\"*20)\n",
    "        accelerator_compute_info(device_idx)                 \n",
    "        accelerator_mem_info(device_idx)\n",
    "        print(\"-\"*20)\n",
    "    # Why is there two cuda mem info ? \"avaialbe and total\" ?\n",
    "    # max_memory=[f'{int(torch.cuda.mem_get_info()[i]/1024**3)-2}GB' for i in range(len(torch.cuda.mem_get_info()))]\n",
    "    # print(f\"max_memory: {max_memory}\")\n",
    "\n",
    "    \n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model download cache directory\n",
    "# DATA_ROOT=\"/data\"\n",
    "DATA_ROOT=\"/home/jovyan/llm-models\"\n",
    "os.environ['XDG_CACHE_HOME']=f\"{DATA_ROOT}/core-kind/yinwang/models\"\n",
    "\n",
    "model_map = {\n",
    "   \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "   \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "   \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "}\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "token_file_path = f\"{DATA_ROOT}/core-kind/yinwang/.cache/huggingface/token\"\n",
    "file = open(token_file_path, \"r\")\n",
    "# file read add a new line to the token, remove it.\n",
    "token = file.read().replace('\\n', '')\n",
    "\n",
    "# print the raw string to see if there is new line in the token\n",
    "# print(r'{}'.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "# model_type = \"13B\"\n",
    "model_type = \"7B\"\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a18b436d-e07e-4c3b-a1f7-e73c602b7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 4 µs, total: 7 µs\n",
      "Wall time: 13.1 µs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506df62b669a44da929dc9c8564b2d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/utils/hub.py:373: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# in Transformer 4.32.1 need to use \"token\" parameter\n",
    "# in Transformer 4.30.x need to use \"use_auth_token\" parameter\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=token,\n",
    "    # use_auth_token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc0308f6-5431-4ed9-af26-b9d0be6c3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "056d0ca9-6fb6-4999-8ba4-979a3d16ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device_name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Multi_processor  : 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 12.615234 GB\n",
      "Allocated memory : 12.613792 GB\n",
      "Free      memory : 0.001442 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# check the available GPU memory after loading the LLM\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(generator: transformers.pipelines.text_generation.TextGenerationPipeline, tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):    \n",
    "    def local(input: str) -> None:\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_length=200,\n",
    "        )\n",
    "        for seq in sequences:\n",
    "            print(f\"Result: \\n{seq['generated_text']}\")\n",
    "\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        print(\"-\"*20)\n",
    "        print(f\"walltime: {duration} in secs.\")\n",
    "        gpu_usage()\n",
    "        \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bfad65b-f23b-4e99-b724-57dd530b99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chain of thoughts prompting\n",
    "input='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: The cafeteria started with 23 apples. They used 20 to make lunch, leaving 3 apples. They bought 6 more, so now they have 3 + 6 = 9 apples. The answer is 9.\n",
      "Q: A pizza parlor has 12 large pizzas to deliver. If they\n",
      "--------------------\n",
      "walltime: 3.8247992992401123 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device_name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Multi_processor  : 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 12.974609 GB\n",
      "Allocated memory : 12.621727 GB\n",
      "Free      memory : 0.352882 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "chat(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d410207-f7de-4ebf-80cc-436217ee8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_text_loader import PDFHelper\n",
    "\n",
    "loader = PDFHelper(data_folder = \"./data/medreports\", file_pattern=\"KK-SCIVIAS-*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1942d22b-8e04-4d37-9e08-fe3f25e5d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = loader.read_pdf(1)\n",
    "input = f\"Context: Patient: Fried\\nFrage: Welcher name hat der Patient?\\nAntwort: Name ist Fried\\nContext: {context}\\nFrage: Welcher name hat die Patientin?\\nAntwort: die Patientin hat name \"\n",
    "\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881162a-6514-4f31-a64c-60718b2f6338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
