{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a134cd-c9b0-4cdb-abf0-1b2622184a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608244d5-b5e5-4740-b44e-81ea0d67ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48662e9c-04da-455f-a80d-bd8136ee5f2e",
   "metadata": {},
   "source": [
    "#### Useful installation for KF notebook 1.7.0 cu111 drivers\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade transformers==4.31.0\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==1.10.2+cu111 fastai==2.7.12 fastcore==1.5.29 fastdownload==0.0.7 torchvision==0.11.3+cu111 --extra-index-url https://download.pytorch.org/whl/cu111\n",
    "#!{sys.executable} -m pip install --user --upgrade accelerate==0.20.3\n",
    "```\n",
    "cuda118\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==2.0.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "`xformers==0.0.21` need `torch==2.0.1``\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade xformers==0.0.21 torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "```\n",
    "\n",
    "show js loading with ipywidgets\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0 comm==0.1.4 jupyterlab-widgets==3.0.8 widgetsnbextension==4.0.8\n",
    "```\n",
    "\n",
    "uninstall\n",
    "```shell\n",
    "#!{sys.executable} -m pip uninstall accelerator transformers xformers torch -y \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92d003-03af-4750-9a15-85f2eb2cffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list=!nvidia-smi -L\n",
    "for i in range(len(list)):\n",
    "    print(list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76206acf-2f57-4460-b0cd-d94af56fa07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_device_uuid(input: str) -> str:\n",
    "    try:\n",
    "        # r'' before the search pattern indicates it is a raw string, \n",
    "        # otherwise \"\" instead of single quote\n",
    "        uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "    except AttributeError:\n",
    "        # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "        uuid = \"\"\n",
    "    return uuid    \n",
    "\n",
    "# skip the first GPU ID, only get the MIG IDs, using python list slice over index access\n",
    "uuid_list = [get_device_uuid(e) for e in list[1:]]\n",
    "# print(uuid_list)\n",
    "UUIDs = \",\".join(uuid_list)\n",
    "print(UUIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys\n",
    "from platform import python_version\n",
    "# data volume mounted in kubeflow notebook\n",
    "DATA_ROOT=\"/home/jovyan/llm-models\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = UUIDs # \"0,1,2\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\" # for debugging\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "os.environ['XDG_CACHE_HOME']=f\"{DATA_ROOT}/core-kind/yinwang/models\"\n",
    "\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e320b2a-cac5-421a-abd5-036c6212b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0dfd5-9a5a-460a-a723-e62cad74536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/58216000/get-total-amount-of-free-gpu-memory-and-available-using-pytorch\n",
    "# torch.cuda.device_count()\n",
    "# t = torch.cuda.get_device_properties(0).total_memory\n",
    "# r = torch.cuda.memory_reserved(0)\n",
    "# a = torch.cuda.memory_allocated(0)\n",
    "# f = r-a  # free inside reserved\n",
    "# print(t/1024**3)\n",
    "# print(r/1024**3)\n",
    "# print(a/1024**3)\n",
    "# print(f/1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff43674-5abd-4da1-88d5-8c7daa6aca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/58216000/get-total-amount-of-free-gpu-memory-and-available-using-pytorch\n",
    "# from typing import Tuple\n",
    "\n",
    "def byte_gb_info(byte_mem) -> str:\n",
    "    \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "    # format the f string float with :.2f to decimal digits\n",
    "    # https://zetcode.com/python/fstring/\n",
    "    return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "def accelerator_mem_info(device_idx: int):\n",
    "    # total\n",
    "    t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "    # usable\n",
    "    r = torch.cuda.memory_reserved(device_idx)\n",
    "    # allocated\n",
    "    a = torch.cuda.memory_allocated(device_idx)\n",
    "    # still free\n",
    "    f = r-a\n",
    "    # unit = \"GB\"   \n",
    "    print( # \"GPU memory info:\\n\" + \n",
    "          f\"Physical  memory : {byte_gb_info(t)}\\n\" + \n",
    "          f\"Reserved  memory : {byte_gb_info(r)}\\n\" + \n",
    "          f\"Allocated memory : {byte_gb_info(a)}\\n\" + \n",
    "          f\"Free      memory : {byte_gb_info(f)}\")\n",
    "\n",
    "    \n",
    "def accelerator_compute_info(device_idx: int):\n",
    "    name = torch.cuda.get_device_properties(device_idx).name\n",
    "    count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "    print(f\"Device_name      : {name} \\n\" +\n",
    "          f\"Multi_processor  : {count}\")    \n",
    "\n",
    "    \n",
    "def gpu_usage():        \n",
    "    num_of_gpus = torch.cuda.device_count();\n",
    "    # this shows only the gpu device, not the MIG\n",
    "    print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "    # available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "    # available_gpus = [torch.cuda.get_device_properties(i).name for i in range(torch.cuda.device_count())]\n",
    "    # print(f\"device_mig_info: {available_gpus}\")\n",
    "    for device_idx in range(torch.cuda.device_count()):\n",
    "        print(\"-\"*20)\n",
    "        accelerator_compute_info(device_idx)                 \n",
    "        accelerator_mem_info(device_idx)\n",
    "        print(\"-\"*20)\n",
    "    # Why is there two cuda mem info ? \"avaialbe and total\" ?\n",
    "    # max_memory=[f'{int(torch.cuda.mem_get_info()[i]/1024**3)-2}GB' for i in range(len(torch.cuda.mem_get_info()))]\n",
    "    # print(f\"max_memory: {max_memory}\")\n",
    "\n",
    "    \n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model download cache directory\n",
    "# DATA_ROOT=\"/data\"\n",
    "# DATA_ROOT=\"/home/jovyan/llm-models\"\n",
    "# os.environ['XDG_CACHE_HOME']=f\"{DATA_ROOT}/core-kind/yinwang/models\"\n",
    "\n",
    "model_map = {\n",
    "        \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"70B\" : \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        # \"70B\" : \"meta-llama/Llama-2-70b-hf\" \n",
    "}\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "token_file_path = f\"{DATA_ROOT}/core-kind/yinwang/.cache/huggingface/token\"\n",
    "file = open(token_file_path, \"r\")\n",
    "# file read add a new line to the token, remove it.\n",
    "token = file.read().replace('\\n', '')\n",
    "file.close()\n",
    "\n",
    "# print the raw string to see if there is new line in the token\n",
    "# print(r'{}'.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = \"13B\"\n",
    "model_type = \"7B\"\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    #token=token, #transformer==4.32.1\n",
    "    use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b436d-e07e-4c3b-a1f7-e73c602b7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# in Transformer 4.32.1 need to use \"token\" parameter\n",
    "# in Transformer 4.30.x need to use \"use_auth_token\" parameter\n",
    "# with torch.no_grad():\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    #token=token, #transformer==4.32.1\n",
    "    use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0308f6-5431-4ed9-af26-b9d0be6c3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d0ca9-6fb6-4999-8ba4-979a3d16ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the available GPU memory after loading the LLM\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast\n",
    "):    \n",
    "    def local(input: str, print_mode: bool = True) -> list:\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens=200,\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        result = []\n",
    "        for seq in sequences:\n",
    "            result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        if print_mode == True:\n",
    "            for s in result:\n",
    "                print(s)\n",
    "\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_usage() \n",
    "        return result\n",
    "            \n",
    "    \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775ca5a-1974-4837-8821-b556266cee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set DEBUG to false to remove all the llm answer outputs\n",
    "# DEBUG=True\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_answer(answer: list)-> None:\n",
    "    if DEBUG:\n",
    "        print(\"-\"*10)\n",
    "        print(answer[0])\n",
    "        print(\"-\"*10)\n",
    "        print(answer[0].split(\"\\n\")[-1])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64be04-c528-426b-91f1-47b71996d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def free_memory_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def local():\n",
    "        l_generator = generator\n",
    "        l_tokenizer = tokenizer\n",
    "        #l_generator.cpu()\n",
    "        #l_tokenizer.cpu()\n",
    "        # model.cpu()\n",
    "        \n",
    "        del l_tokenizer, l_generator\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        #for device_idx in range(torch.cuda.device_count()):\n",
    "        #    print(device_idx)\n",
    "        #    device = torch.device(f\"cuda:{device_idx}\")\n",
    "        #    device.reset()\n",
    "    return local    \n",
    "\n",
    "free_memory = free_memory_gen(generator, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfad65b-f23b-4e99-b724-57dd530b99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of thoughts prompting\n",
    "\n",
    "# testing prompt\n",
    "input='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat(input, False)\n",
    "#print_answer(answer)\n",
    "print(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d410207-f7de-4ebf-80cc-436217ee8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_text_loader import PDFHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc455c76-8203-4541-a82e-80444bf1835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = PDFHelper(data_folder = \"./data/medreports\", file_pattern=\"KK-SCIVIAS-*.pdf\")\n",
    "#context = loader.read_pdf(1)\n",
    "\n",
    "#input = f\"Context: Patient: Fried\\nFrage: Welcher name hat der Patient?\\nAntwort: Name ist Fried\\nContext: {context}\\nFrage: Welcher name hat die Patientin?\\nAntwort: die Patientin hat name \"\n",
    "#print(input)\n",
    "#chat(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942d22b-8e04-4d37-9e08-fe3f25e5d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"core-kind/yinwang\"\n",
    "loader = PDFHelper(data_folder = f\"{DATA_ROOT}/{data_path}/data/medreports\", file_pattern=\"KK-SCIVIAS-*.txt\")\n",
    "context = loader.read_txt(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861587f9-f41e-4ce3-903b-ec50b17f4cce",
   "metadata": {},
   "source": [
    "#### zero shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5316fa-82d8-4f47-98fa-9506916a1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name\n",
    "input=f\"Can you tell me the name of the patient from the folowing doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6b804-67bd-4c54-a7c2-0e5ef14aa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input)\n",
    "# 6810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a0f12-7d99-4aba-9c8a-5fb1e28f456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c299015-c5b7-425b-8a79-ba155491c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age\n",
    "input=f\"Can you tell me the age of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf966a-76c1-4e7d-8a6d-0b356b2b86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d472f-80db-4d73-ac73-f673bc5fe279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnosis\n",
    "input=f\"Can you tell me the diagnosis of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7927d-9cba-4cbf-a05c-4fcaf886c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa11f-0eb2-4dac-aa70-194cfe8e577d",
   "metadata": {},
   "source": [
    "#### Chain-of-thoughts prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name prompt\n",
    "input = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {context}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881162a-6514-4f31-a64c-60718b2f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61eea4-3731-4ba5-8c1b-e392cc6fe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "input = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{context}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "# print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31ec3d-1912-4a07-aab0-c84151783112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "#len(input)\n",
    "# > 6913 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbeb59-1fe4-4ab0-b004-05b2fe2d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a401151-af8b-46f4-8ef2-0177ae075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose prompt\n",
    "input=f\"Context:\\nPatient: Fried is a 34-year-old patient, Diagnoses: Influenza (J09.X2) \\nQuestion:\\nWhat diagnoses has the patient? \\nAnswer:\\nFried is a patient, 34 year-old, has diagnoses Influenza (J09.X2). The answers is Influenza (J09.X2)\\nContext:\\n{context}\\nQuestion:\\nWhat diagnoses has the patient?\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd94444-a458-40dd-b95d-2be26f9ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)\n",
    "print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b6e3c-3d58-4cf6-b834-1d066519e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d74a3-4ef3-4379-9075-a3944812244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4aca2a-e1cc-4769-bdd8-268906d978d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4464c-2597-4eed-b3b1-16e67d966fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
