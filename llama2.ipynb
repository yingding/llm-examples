{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84683eed-edfb-4c7a-94dc-9b845a585985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade kfp==2.0.0b13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae1027a-6b43-4efa-a5af-582448b845dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the ipywidgets and restart kernel the javascript widget for Huggingface download widget will show.\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbafbba5-b1fc-429f-b792-e829e0e5ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea92d003-03af-4750-9a15-85f2eb2cffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-51f84540-9ebb-1d44-7bb7-3c62ae55c20e)\n",
      "  MIG 2g.20gb     Device  0: (UUID: MIG-0efc9f06-6dca-5886-98af-0273ca7fde51)\n"
     ]
    }
   ],
   "source": [
    "list=!nvidia-smi -L\n",
    "for i in range(len(list)):\n",
    "    print(list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76206acf-2f57-4460-b0cd-d94af56fa07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-0efc9f06-6dca-5886-98af-0273ca7fde51\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_device_uuid(input: str) -> str:\n",
    "    try:\n",
    "        # r'' before the search pattern indicates it is a raw string, \n",
    "        # otherwise \"\" instead of single quote\n",
    "        uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "    except AttributeError:\n",
    "        # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "        uuid = \"\"\n",
    "    return uuid    \n",
    "\n",
    "# skip the first GPU ID, only get the MIG IDs, using python list slice over index access\n",
    "uuid_list = [get_device_uuid(e) for e in list[1:]]\n",
    "# print(uuid_list)\n",
    "UUIDs = \",\".join(uuid_list)\n",
    "print(UUIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-0efc9f06-6dca-5886-98af-0273ca7fde51\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from platform import python_version\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = UUIDs # \"0,1,2\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e320b2a-cac5-421a-abd5-036c6212b612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(20748107776, 20937965568)\n",
      "19.32318115234375\n",
      "19.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c0dfd5-9a5a-460a-a723-e62cad74536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/58216000/get-total-amount-of-free-gpu-memory-and-available-using-pytorch\n",
    "# torch.cuda.device_count()\n",
    "# t = torch.cuda.get_device_properties(0).total_memory\n",
    "# r = torch.cuda.memory_reserved(0)\n",
    "# a = torch.cuda.memory_allocated(0)\n",
    "# f = r-a  # free inside reserved\n",
    "# print(t/1024**3)\n",
    "# print(r/1024**3)\n",
    "# print(a/1024**3)\n",
    "# print(f/1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ff43674-5abd-4da1-88d5-8c7daa6aca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device_name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Multi_processor  : 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://stackoverflow.com/questions/58216000/get-total-amount-of-free-gpu-memory-and-available-using-pytorch\n",
    "# from typing import Tuple\n",
    "\n",
    "def byte_gb_info(byte_mem) -> str:\n",
    "    \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "    # format the f string float with :.2f to decimal digits\n",
    "    # https://zetcode.com/python/fstring/\n",
    "    return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "def accelerator_mem_info(device_idx: int):\n",
    "    # total\n",
    "    t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "    # usable\n",
    "    r = torch.cuda.memory_reserved(device_idx)\n",
    "    # allocated\n",
    "    a = torch.cuda.memory_allocated(device_idx)\n",
    "    # still free\n",
    "    f = r-a\n",
    "    # unit = \"GB\"   \n",
    "    print( # \"GPU memory info:\\n\" + \n",
    "          f\"Physical  memory : {byte_gb_info(t)}\\n\" + \n",
    "          f\"Reserved  memory : {byte_gb_info(r)}\\n\" + \n",
    "          f\"Allocated memory : {byte_gb_info(a)}\\n\" + \n",
    "          f\"Free      memory : {byte_gb_info(f)}\")\n",
    "\n",
    "    \n",
    "def accelerator_compute_info(device_idx: int):\n",
    "    name = torch.cuda.get_device_properties(device_idx).name\n",
    "    count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "    print(f\"Device_name      : {name} \\n\" +\n",
    "          f\"Multi_processor  : {count}\")    \n",
    "\n",
    "    \n",
    "def gpu_usage():        \n",
    "    num_of_gpus = torch.cuda.device_count();\n",
    "    # this shows only the gpu device, not the MIG\n",
    "    print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "    # available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "    # available_gpus = [torch.cuda.get_device_properties(i).name for i in range(torch.cuda.device_count())]\n",
    "    # print(f\"device_mig_info: {available_gpus}\")\n",
    "    for device_idx in range(torch.cuda.device_count()):\n",
    "        print(\"-\"*20)\n",
    "        accelerator_compute_info(device_idx)                 \n",
    "        accelerator_mem_info(device_idx)\n",
    "        print(\"-\"*20)\n",
    "    # Why is there two cuda mem info ? \"avaialbe and total\" ?\n",
    "    # max_memory=[f'{int(torch.cuda.mem_get_info()[i]/1024**3)-2}GB' for i in range(len(torch.cuda.mem_get_info()))]\n",
    "    # print(f\"max_memory: {max_memory}\")\n",
    "\n",
    "    \n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model download cache directory\n",
    "# DATA_ROOT=\"/data\"\n",
    "DATA_ROOT=\"/home/jovyan/llm-models\"\n",
    "os.environ['XDG_CACHE_HOME']=f\"{DATA_ROOT}/core-kind/yinwang/models\"\n",
    "\n",
    "model_map = {\n",
    "   \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "   \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "   \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "}\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "token_file_path = f\"{DATA_ROOT}/core-kind/yinwang/.cache/huggingface/token\"\n",
    "file = open(token_file_path, \"r\")\n",
    "# file read add a new line to the token, remove it.\n",
    "token = file.read().replace('\\n', '')\n",
    "file.close()\n",
    "\n",
    "# print the raw string to see if there is new line in the token\n",
    "# print(r'{}'.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "# model_type = \"13B\"\n",
    "model_type = \"7B\"\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a18b436d-e07e-4c3b-a1f7-e73c602b7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 4 µs, total: 7 µs\n",
      "Wall time: 15.5 µs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f88ade83a449c390a0ff500524ea58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# in Transformer 4.32.1 need to use \"token\" parameter\n",
    "# in Transformer 4.30.x need to use \"use_auth_token\" parameter\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=token,\n",
    "    # use_auth_token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc0308f6-5431-4ed9-af26-b9d0be6c3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "056d0ca9-6fb6-4999-8ba4-979a3d16ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device_name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Multi_processor  : 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 12.615234 GB\n",
      "Allocated memory : 12.613792 GB\n",
      "Free      memory : 0.001442 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# check the available GPU memory after loading the LLM\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast\n",
    "):    \n",
    "    def local(input: str, print_mode: bool = True) -> list:\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens=200,\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        result = []\n",
    "        for seq in sequences:\n",
    "            result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        if print_mode == True:\n",
    "            for s in result:\n",
    "                print(s)\n",
    "\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_usage() \n",
    "        else:\n",
    "            return result\n",
    "            \n",
    "    \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bfad65b-f23b-4e99-b724-57dd530b99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chain of thoughts prompting\n",
    "input='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: The cafeteria had 23 apples to start. They used 20 to make lunch, so now they have 23 - 20 = 3 apples left. Then, they bought 6 more, so now they have 3 + 6 = 9 apples. The answer is 9.\n",
      "--------------------\n",
      "walltime: 3.4176883697509766 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device_name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Multi_processor  : 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 12.974609 GB\n",
      "Allocated memory : 12.621727 GB\n",
      "Free      memory : 0.352882 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "chat(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d410207-f7de-4ebf-80cc-436217ee8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_text_loader import PDFHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc455c76-8203-4541-a82e-80444bf1835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = PDFHelper(data_folder = \"./data/medreports\", file_pattern=\"KK-SCIVIAS-*.pdf\")\n",
    "#context = loader.read_pdf(1)\n",
    "\n",
    "#input = f\"Context: Patient: Fried\\nFrage: Welcher name hat der Patient?\\nAntwort: Name ist Fried\\nContext: {context}\\nFrage: Welcher name hat die Patientin?\\nAntwort: die Patientin hat name \"\n",
    "#print(input)\n",
    "#chat(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1942d22b-8e04-4d37-9e08-fe3f25e5d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"core-kind/yinwang\"\n",
    "loader = PDFHelper(data_folder = f\"{DATA_ROOT}/{data_path}/data/medreports\", file_pattern=\"KK-SCIVIAS-*.txt\")\n",
    "context = loader.read_txt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {context}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7881162a-6514-4f31-a64c-60718b2f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "446a1cd1-0091-4ca3-b7db-122625f4a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer[0].split(\"\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e61eea4-3731-4ba5-8c1b-e392cc6fe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{context}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "# print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80fbeb59-1fe4-4ab0-b004-05b2fe2d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=chat(input, print_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a401151-af8b-46f4-8ef2-0177ae075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer[0].split(\"\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987fd94-068e-4a05-bba5-05f4aae26bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
