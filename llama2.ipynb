{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84683eed-edfb-4c7a-94dc-9b845a585985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade kfp==2.0.0b13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae1027a-6b43-4efa-a5af-582448b845dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the ipywidgets and restart kernel the javascript widget for Huggingface download widget will show.\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbafbba5-b1fc-429f-b792-e829e0e5ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea92d003-03af-4750-9a15-85f2eb2cffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-51f84540-9ebb-1d44-7bb7-3c62ae55c20e)\n",
      "  MIG 2g.20gb     Device  0: (UUID: MIG-0efc9f06-6dca-5886-98af-0273ca7fde51)\n"
     ]
    }
   ],
   "source": [
    "list=!nvidia-smi -L\n",
    "for i in range(len(list)):\n",
    "    print(list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76206acf-2f57-4460-b0cd-d94af56fa07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-0efc9f06-6dca-5886-98af-0273ca7fde51\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_device_uuid(input: str) -> str:\n",
    "    try:\n",
    "        # r'' before the search pattern indicates it is a raw string, \n",
    "        # otherwise \"\" instead of single quote\n",
    "        uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "    except AttributeError:\n",
    "        # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "        uuid = \"\"\n",
    "    return uuid    \n",
    "\n",
    "# skip the first GPU ID, only get the MIG IDs, using python list slice over index access\n",
    "uuid_list = [get_device_uuid(e) for e in list[1:]]\n",
    "# print(uuid_list)\n",
    "UUIDs = \",\".join(uuid_list)\n",
    "print(UUIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-0efc9f06-6dca-5886-98af-0273ca7fde51\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from platform import python_version\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = UUIDs # \"0,1,2\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e320b2a-cac5-421a-abd5-036c6212b612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(20748107776, 20937965568)\n",
      "19.32318115234375\n",
      "19.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff43674-5abd-4da1-88d5-8c7daa6aca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "device_mig_info: ['NVIDIA A100 80GB PCIe MIG 2g.20gb']\n",
      "max_memory: ['17GB', '17GB']\n"
     ]
    }
   ],
   "source": [
    " def gpu_usage():\n",
    "    num_of_gpus = torch.cuda.device_count();\n",
    "    # this shows only the gpu device, not the MIG\n",
    "    print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "    # available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "    available_gpus = [torch.cuda.get_device_properties(i).name for i in range(torch.cuda.device_count())]\n",
    "    print(f\"device_mig_info: {available_gpus}\")\n",
    "    # Why is there two cuda mem info ? \"avaialbe and total\" ?\n",
    "    max_memory=[f'{int(torch.cuda.mem_get_info()[i]/1024**3)-2}GB' for i in range(len(torch.cuda.mem_get_info()))]\n",
    "    print(f\"max_memory: {max_memory}\")\n",
    "\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model download cache directory\n",
    "# DATA_ROOT=\"/data\"\n",
    "DATA_ROOT=\"/home/jovyan/llm-models\"\n",
    "os.environ['XDG_CACHE_HOME']=f\"{DATA_ROOT}/core-kind/yinwang/models\"\n",
    "\n",
    "model_map = {\n",
    "   \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "   \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "   \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "}\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "token_file_path = f\"{DATA_ROOT}/core-kind/yinwang/.cache/huggingface/token\"\n",
    "file = open(token_file_path, \"r\")\n",
    "# file read add a new line to the token, remove it.\n",
    "token = file.read().replace('\\n', '')\n",
    "\n",
    "# print the raw string to see if there is new line in the token\n",
    "# print(r'{}'.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-13b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "model_type = \"13B\"\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a18b436d-e07e-4c3b-a1f7-e73c602b7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba83fd57ba941eb8e7273312ce0aca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/utils/hub.py:373: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# in Transformer 4.32.1 need to use \"token\" parameter\n",
    "# in Transformer 4.30.x need to use \"use_auth_token\" parameter\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=token,\n",
    "    # use_auth_token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc0308f6-5431-4ed9-af26-b9d0be6c3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056d0ca9-6fb6-4999-8ba4-979a3d16ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "device_mig_info: ['NVIDIA A100 80GB PCIe MIG 2g.20gb']\n",
      "max_memory: ['0GB', '17GB']\n"
     ]
    }
   ],
   "source": [
    "# check the available GPU memory after loading the LLM\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(generator: transformers.pipelines.text_generation.TextGenerationPipeline, tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):    \n",
    "    def local(input: str) -> None:\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_length=200,\n",
    "        )\n",
    "        for seq in sequences:\n",
    "            print(f\"Result: {seq['generated_text']}\")\n",
    "\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        print(\"-\"*20)\n",
    "        print(f\"walltime: {duration} in secs.\")\n",
    "        gpu_usage()\n",
    "        \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bfad65b-f23b-4e99-b724-57dd530b99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: They started with 23 apples. 20 apples were used for lunch. That leaves 23 - 20 = 3 apples left. Then they bought 6 more apples. 3 + 6 = 9. The answer is 9.\n",
      "Q: If a bookshelf has 12 shelves, and\n",
      "--------------------\n",
      "walltime: 59.21881556510925 in secs.\n",
      "num_of_gpus: 1\n",
      "device_mig_info: ['NVIDIA A100 80GB PCIe MIG 2g.20gb']\n",
      "max_memory: ['0GB', '17GB']\n"
     ]
    }
   ],
   "source": [
    "chat(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d410207-f7de-4ebf-80cc-436217ee8206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
