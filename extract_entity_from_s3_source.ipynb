{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4946d3e4-c86a-497c-a0e4-21c89b93799d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@created: 24.11.2023\\\n",
    "@updated: 27.12.2023\\\n",
    "@version: 4\n",
    "\n",
    "This notebook comprises of examples to use transformer, pytorch, llama2, langchain to achive entity extraction with engineered prompts.\n",
    "\n",
    "Note:\\\n",
    "`RuntimeError:NVML_SICCESS ... in PyTorch using the Llama2 13B means OOM with 20GB MIG`\n",
    "https://github.com/pytorch/pytorch/issues/112377\n",
    "\n",
    "Use `nvidia.com/mig-3g.40gb` instead of `nvidia.com/mig-2g.20gb` solves the issue.\n",
    "\n",
    "In the map reduce RAG pattern, ran into recency bias in the reduce pattern. Due to the previous hallucination.\\\n",
    "Possible solution\n",
    "* add end token in the map steps\n",
    "* use rerank pattern\n",
    "* better rag retrieval and embedding model\n",
    "* large rag chunck\n",
    "\n",
    "Question:\n",
    "* is there a \"end\" special token for llama2 https://www.reddit.com/r/LocalLLaMA/comments/167v3cd/llama_2_tokenizer_and_the_special_tokens/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04aee56a-508c-4010-b447-f6ab9db29e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pydantic_core is from 2.xx version doesn't work with docarray 0.39.1\n",
    "# !{sys.executable} -m pip uninstall pydantic_core -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install --user --upgrade kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac05d7-4f2e-4fc0-99f8-bca2ffffe3cc",
   "metadata": {},
   "source": [
    "## Use the cuda 118 and torch 2.1.0 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a925dce-3bdf-4a58-9661-7715a977c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: huggingface_hub==0.20.2 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 1)) (0.20.2)\n",
      "Requirement already satisfied: transformers==4.36.2 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 4)) (4.36.2)\n",
      "Requirement already satisfied: urllib3==1.26.16 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 12)) (1.26.16)\n",
      "Requirement already satisfied: jsonschema==4.19.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 13)) (4.19.0)\n",
      "Requirement already satisfied: fastai==2.7.13 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 14)) (2.7.13)\n",
      "Requirement already satisfied: ipywidgets==8.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 16)) (8.1.0)\n",
      "Requirement already satisfied: click==8.1.7 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 18)) (8.1.7)\n",
      "Requirement already satisfied: multipledispatch==1.0.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 30)) (1.0.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.99 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 32)) (0.1.99)\n",
      "Requirement already satisfied: sacremoses==0.0.53 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 35)) (0.0.53)\n",
      "Requirement already satisfied: pypdf==3.15.5 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 37)) (3.15.5)\n",
      "Requirement already satisfied: accelerate==0.24.1 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 44)) (0.24.1)\n",
      "Requirement already satisfied: torch==2.1.0+cu118 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 48)) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchaudio==2.1.0+cu118 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 49)) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision==0.16.0+cu118 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 50)) (0.16.0+cu118)\n",
      "Requirement already satisfied: xformers==0.0.22.post7+cu118 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 51)) (0.0.22.post7+cu118)\n",
      "Requirement already satisfied: langchain==0.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 62)) (0.1.0)\n",
      "Requirement already satisfied: pydantic==1.10.13 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 67)) (1.10.13)\n",
      "Requirement already satisfied: unstructured==0.11.0 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 70)) (0.11.0)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 71)) (2.2.2)\n",
      "Requirement already satisfied: docarray==0.39.1 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 72)) (0.39.1)\n",
      "Requirement already satisfied: boto3==1.34.14 in /home/jovyan/.local/lib/python3.8/site-packages (from -r ./requirements.txt (line 78)) (1.34.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (2023.9.0)\n",
      "Requirement already satisfied: requests in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.8/site-packages (from huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (3.12.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.36.2->-r ./requirements.txt (line 4)) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.36.2->-r ./requirements.txt (line 4)) (1.22.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.36.2->-r ./requirements.txt (line 4)) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/jovyan/.local/lib/python3.8/site-packages (from transformers==4.36.2->-r ./requirements.txt (line 4)) (0.15.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 13)) (0.10.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 13)) (22.2.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 13)) (0.30.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 13)) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 13)) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema==4.19.0->-r ./requirements.txt (line 13)) (5.12.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (1.7.0)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /home/jovyan/.local/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (1.5.29)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (3.4.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (1.2.4)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (1.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (9.4.0)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (23.0.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (0.24.2)\n",
      "Requirement already satisfied: spacy<4 in /home/jovyan/.local/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (3.7.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /home/jovyan/.local/lib/python3.8/site-packages (from fastai==2.7.13->-r ./requirements.txt (line 14)) (0.0.7)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (8.11.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (3.0.8)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (4.0.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jovyan/.local/lib/python3.8/site-packages (from ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.1.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses==0.0.53->-r ./requirements.txt (line 35)) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses==0.0.53->-r ./requirements.txt (line 35)) (1.2.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate==0.24.1->-r ./requirements.txt (line 44)) (5.9.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch==2.1.0+cu118->-r ./requirements.txt (line 48)) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.1.0+cu118->-r ./requirements.txt (line 48)) (2.1.0)\n",
      "Requirement already satisfied: sympy in /home/jovyan/.local/lib/python3.8/site-packages (from torch==2.1.0+cu118->-r ./requirements.txt (line 48)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch==2.1.0+cu118->-r ./requirements.txt (line 48)) (3.0)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (0.1.8)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (0.0.10)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (0.0.77)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (4.0.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (1.33)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (2.0.24)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/jovyan/.local/lib/python3.8/site-packages (from langchain==0.1.0->-r ./requirements.txt (line 62)) (0.6.2)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (1.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (4.12.2)\n",
      "Requirement already satisfied: emoji in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (2.8.0)\n",
      "Requirement already satisfied: rapidfuzz in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (3.5.2)\n",
      "Requirement already satisfied: python-magic in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (0.4.27)\n",
      "Requirement already satisfied: nltk in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (3.8.1)\n",
      "Requirement already satisfied: filetype in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (1.2.0)\n",
      "Requirement already satisfied: chardet in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (5.2.0)\n",
      "Requirement already satisfied: lxml in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (4.9.3)\n",
      "Requirement already satisfied: langdetect in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (1.0.9)\n",
      "Requirement already satisfied: backoff in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (2.2.1)\n",
      "Requirement already satisfied: python-iso639 in /home/jovyan/.local/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (2023.6.15)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from unstructured==0.11.0->-r ./requirements.txt (line 70)) (0.9.0)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /home/jovyan/.local/lib/python3.8/site-packages (from docarray==0.39.1->-r ./requirements.txt (line 72)) (2.31.0.6)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /home/jovyan/.local/lib/python3.8/site-packages (from docarray==0.39.1->-r ./requirements.txt (line 72)) (3.9.10)\n",
      "Requirement already satisfied: rich>=13.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from docarray==0.39.1->-r ./requirements.txt (line 72)) (13.7.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/jovyan/.local/lib/python3.8/site-packages (from docarray==0.39.1->-r ./requirements.txt (line 72)) (0.9.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.14 in /home/jovyan/.local/lib/python3.8/site-packages (from boto3==1.34.14->-r ./requirements.txt (line 78)) (1.34.14)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/jovyan/.local/lib/python3.8/site-packages (from boto3==1.34.14->-r ./requirements.txt (line 78)) (0.10.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/jovyan/.local/lib/python3.8/site-packages (from boto3==1.34.14->-r ./requirements.txt (line 78)) (1.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jovyan/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r ./requirements.txt (line 62)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jovyan/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r ./requirements.txt (line 62)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jovyan/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r ./requirements.txt (line 62)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r ./requirements.txt (line 62)) (1.9.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.35.0,>=1.34.14->boto3==1.34.14->-r ./requirements.txt (line 78)) (2.8.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jovyan/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0->-r ./requirements.txt (line 62)) (3.20.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema==4.19.0->-r ./requirements.txt (line 13)) (3.15.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (3.0.38)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.18.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (4.8.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (2.14.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (5.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jovyan/.local/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.0->-r ./requirements.txt (line 62)) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.8/site-packages (from langchain-core<0.2,>=0.1.7->langchain==0.1.0->-r ./requirements.txt (line 62)) (3.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub==0.20.2->-r ./requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jovyan/.local/lib/python3.8/site-packages (from rich>=13.1.0->docarray==0.39.1->-r ./requirements.txt (line 72)) (3.0.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (8.2.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (67.6.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (2.4.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (1.1.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (0.3.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (3.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (0.7.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.8/site-packages (from spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (2.0.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/jovyan/.local/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.0->-r ./requirements.txt (line 62)) (3.0.1)\n",
      "Requirement already satisfied: types-urllib3 in /home/jovyan/.local/lib/python3.8/site-packages (from types-requests>=2.28.11.6->docarray==0.39.1->-r ./requirements.txt (line 72)) (1.26.25.14)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jovyan/.local/lib/python3.8/site-packages (from typing-inspect>=0.8.0->docarray==0.39.1->-r ./requirements.txt (line 72)) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->unstructured==0.11.0->-r ./requirements.txt (line 70)) (2.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch==2.1.0+cu118->-r ./requirements.txt (line 48)) (2.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.13->-r ./requirements.txt (line 14)) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.13->-r ./requirements.txt (line 14)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->fastai==2.7.13->-r ./requirements.txt (line 14)) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->fastai==2.7.13->-r ./requirements.txt (line 14)) (2023.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->fastai==2.7.13->-r ./requirements.txt (line 14)) (3.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jovyan/.local/lib/python3.8/site-packages (from sympy->torch==2.1.0+cu118->-r ./requirements.txt (line 48)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.8/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain==0.1.0->-r ./requirements.txt (line 62)) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray==0.39.1->-r ./requirements.txt (line 72)) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.2.6)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (0.7.9)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/jovyan/.local/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai==2.7.13->-r ./requirements.txt (line 14)) (0.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.0->-r ./requirements.txt (line 16)) (0.2.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48662e9c-04da-455f-a80d-bd8136ee5f2e",
   "metadata": {},
   "source": [
    "## Additional technical informaiton\n",
    "#### Useful installation for KF notebook 1.7.0 cu111 drivers\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade transformers==4.31.0\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==1.10.2+cu111 fastai==2.7.12 fastcore==1.5.29 fastdownload==0.0.7 torchvision==0.11.3+cu111 --extra-index-url https://download.pytorch.org/whl/cu111\n",
    "#!{sys.executable} -m pip install --user --upgrade accelerate==0.20.3\n",
    "```\n",
    "cuda118\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==2.0.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "`xformers==0.0.21` need `torch==2.0.1`\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade xformers==0.0.21 torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "```\n",
    "\n",
    "show js loading with ipywidgets\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0 comm==0.1.4 jupyterlab-widgets==3.0.8 widgetsnbextension==4.0.8\n",
    "```\n",
    "\n",
    "uninstall\n",
    "```shell\n",
    "#!{sys.executable} -m pip uninstall accelerator transformers xformers torch -y \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "#### Basics of GPU\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "#### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "#### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2594fcbd-b0a4-42e0-8ce8-e524423d7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from util.accelerator_utils import AcceleratorStatus, AcceleratorHelper\n",
    "\n",
    "# data volume mounted in kubeflow notebook\n",
    "MODEL_ROOT=\"/home/jovyan/llm-models\"\n",
    "MODEL_SUB_PATH = \"core-kind/yinwang\"\n",
    "# the cache dir for huggingface models\n",
    "MODEL_CACHE_DIR = f\"{MODEL_ROOT}/{MODEL_SUB_PATH}\"\n",
    "\n",
    "gpu_status = AcceleratorStatus()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a268056f-65d5-4e2c-a73a-75570990bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_helper = AcceleratorHelper()\n",
    "# dynamically fetch attached accelerator devices\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd3f3cf-f74e-4377-964d-a5c0c1b667bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-9579f618-ddae-5958-9285-3207382f0b36\n",
      "/home/jovyan/llm-models/core-kind/yinwang/models\n"
     ]
    }
   ],
   "source": [
    "# init all the cuda torch env and model download cache directory\n",
    "gpu_helper.init_cuda_torch(UUIDs, MODEL_CACHE_DIR)\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(os.environ[\"XDG_CACHE_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n",
      "2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "model_map = {\n",
    "        \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"70B\" : \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        # \"70B\" : \"meta-llama/Llama-2-70b-hf\" \n",
    "}\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "# from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the huggingface hub token\n",
    "\"\"\"\n",
    "token_sub_path = \".cache/huggingface/token\"\n",
    "token_file_path = f\"{MODEL_CACHE_DIR}/{token_sub_path}\"\n",
    "# stripe the leading and tailing EOL chars\n",
    "# https://stackoverflow.com/questions/275018/how-can-i-remove-a-trailing-newline/275025#275025\n",
    "with open (token_file_path, \"r\") as file:\n",
    "    # file read add a new line to the token, remove it.\n",
    "    # token = file.read().replace('\\n', '')    \n",
    "    token = file.read().strip()\n",
    "\n",
    "# print the raw string to see if there is new line in the token\n",
    "# print(r'{}'.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-13b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "model_type = \"13B\"\n",
    "# model_type = \"7B\"\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253126b-1dd9-4b0e-8c44-9c5df3191492",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lama2 max position embeddings\n",
    "Default is set to be 2048\n",
    "* https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig.max_position_embeddings\n",
    "\n",
    "Set teh max_length for the tokenizer, Transformer issues:\n",
    "* https://github.com/huggingface/transformers/issues/1791#issuecomment-553397054\n",
    "* https://github.com/huggingface/transformers/pull/1833\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POSITION_EMBEDDINGS = 3072\n",
    "MAX_LENGTH = 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    token=token, #transformer>=4.32.1\n",
    "    device='cpu',\n",
    "    max_position_embeddings=MAX_LENGTH,\n",
    "    max_length=MAX_LENGTH,\n",
    "    # device_map=\"auto\", # put to GPU\n",
    "    # use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc864b9a-eac7-4335-bbdc-a711b8b664b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f43af-d423-436d-95e2-b45465ed5be2",
   "metadata": {},
   "source": [
    "#### Testing tokenizer\n",
    "https://huggingface.co/docs/tokenizers/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "767706d4-ffd0-4e73-aec2-e18de0eb617c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55be8858-a5ce-4467-a82e-cc1dc8b8a8db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 660, 29901, 14159, 756, 29871, 29941, 22556, 26563, 29889, 940, 1321, 952, 29871, 29906, 901, 508, 29879, 310, 22556, 26563, 29889, 7806, 508, 756, 29871, 29946, 22556, 26563, 29889, 1128, 1784, 22556, 26563, 947, 540, 505, 1286, 29973, 13, 29909, 29901, 14159, 4687, 411, 29871, 29941, 26563, 29889, 29871, 29906, 508, 29879, 310, 29871, 29946, 22556, 26563, 1269, 338, 29871, 29947, 22556, 26563, 29889, 29871, 29941, 718, 29871, 29947, 353, 29871, 29896, 29896, 29889, 450, 1234, 338, 29871, 29896, 29896, 29889, 13, 29984, 29901, 450, 274, 2142, 1308, 423, 750, 29871, 29906, 29941, 623, 793, 29889, 960, 896, 1304, 29871, 29906, 29900, 304, 1207, 301, 3322, 322, 18093, 29871, 29953, 901, 29892, 920, 1784, 623, 793, 437, 896, 505, 29973, 13]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f725e85f-ceab-4c4d-8efe-03fbf089a2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd687825-b00c-4909-95f9-373e8f1bdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "# not loading to the GPU with accelerator\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b07f9052-1ad4-41ff-af51-3bd4394e4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # will call the AutoModelForCausalLM automatically\n",
    "# generator = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     token=token, #transformer>=4.32.1\n",
    "#     #use_auth_token=token, #transformer==4.31.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c182261-d89f-4826-9b67-5c43e89d1ff5",
   "metadata": {},
   "source": [
    "### Inference with transformers pipeline\n",
    "\n",
    "Reference:\n",
    "* https://huggingface.co/docs/transformers/pipeline_tutorial\n",
    "\n",
    "Note:\n",
    "* batch is not activated by default, batch is not necessary faster for `transformers.pipeline`\n",
    "* the max_new_tokens set in the pipeline initialization works with langchain.llms.HuggingFacePipeline, but not as a param for the TextGenerationPipeline \n",
    "\n",
    "max_new_tokens https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 10 µs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca10870d2944280a700a743251cf264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time\n",
    "# in Transformer 4.32.1 need to use \"token\" parameter\n",
    "# in Transformer 4.30.x need to use \"use_auth_token\" parameter\n",
    "# with torch.no_grad():\n",
    "generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    # model=model,\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer, # optional\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    # max_length=MAX_LENGTH,\n",
    "    max_length=None, # remove the total length of the generated response\n",
    "    max_new_tokens=100, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "    token=token, #transformer>=4.32.1\n",
    "    #use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77b83ae3-e8b1-4be6-b2e3-7819a9b5cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24bfaf57-d2e0-464d-bd8e-16eb98a6c4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 24.443359 GB\n",
      "Allocated memory : 24.439268 GB\n",
      "Free      memory : 0.004091 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c3232-c9e7-420a-8769-26dfdfb96b1b",
   "metadata": {},
   "source": [
    "## Passing temparature to the generator for each prompt\n",
    "\n",
    "https://discuss.huggingface.co/t/how-to-set-generation-parameters-for-transformers-pipeline/48837\n",
    "\n",
    "LLama2 chat agent\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-70b-chat-agent.ipynb\n",
    "\n",
    "max_length and max_new_tokens only one need to be set\n",
    "https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG = True\n",
    "# def print_answer(answer: list)-> None:\n",
    "#     if DEBUG:\n",
    "#         print(\"-\"*10)\n",
    "#         print(answer[0])\n",
    "#         print(\"-\"*10)\n",
    "#         print(answer[0].split(\"\\n\")[-1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d64be04-c528-426b-91f1-47b71996d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# def free_memory_gen(\n",
    "#     generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "#     tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     def local():\n",
    "#         l_generator = generator\n",
    "#         l_tokenizer = tokenizer\n",
    "#         #l_generator.cpu()\n",
    "#         #l_tokenizer.cpu()\n",
    "#         # model.cpu()\n",
    "        \n",
    "#         del l_tokenizer, l_generator\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         #for device_idx in range(torch.cuda.device_count()):\n",
    "#         #    print(device_idx)\n",
    "#         #    device = torch.device(f\"cuda:{device_idx}\")\n",
    "#         #    device.reset()\n",
    "#     return local    \n",
    "\n",
    "# free_memory = free_memory_gen(generator, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bfad65b-f23b-4e99-b724-57dd530b99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant.\n",
      "Always answer as helpfully as possible using the context text provided.\n",
      "Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chain of thoughts prompting\n",
    "\n",
    "# system message\n",
    "system_message=\"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "# testing prompt\n",
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']\n",
    "\n",
    "def get_inputs(idx):   \n",
    "    return f\"{system_message}{inputs[idx]}\" \n",
    "\n",
    "# print(inputs[0])\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: They started with 23 apples. 20 were used for lunch, leaving 3. Then they bought 6 more, so they have 3 + 6 = 9 apples.\n",
      "--------------------\n",
      "walltime: 2.1317131519317627 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 24.734375 GB\n",
      "Allocated memory : 24.447203 GB\n",
      "Free      memory : 0.287172 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])\n",
    "    \n",
    "# note: the expected answer is 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a433d07-366b-437f-a164-d7a84947b65e",
   "metadata": {},
   "source": [
    "## Huggingface with Local LLM\n",
    "\n",
    "* https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
    "\n",
    "HuggingFacePipeline from langchain need pydantic>=1.10.13\n",
    "\n",
    "```shell\n",
    "import pydantic\n",
    "print(pydantic.__version__)\n",
    "```\n",
    "* https://stackoverflow.com/questions/76313592/import-langchain-error-typeerror-issubclass-arg-1-must-be-a-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec0ecea0-3c65-4fd4-81c1-9847a6b2e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFacePipeline version 0.0.313 need pydantic >= 1.10.13\n",
    "# HuggingFacePipeline works in version 0.0.312 with pydantic <= 1.10.2\n",
    "\n",
    "# !{sys.executable} -m pip install --user --upgrade langchain==0.0.341\n",
    "# !{sys.executable} -m pip install --user --upgrade langchain==0.0.312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afb42d96-a471-4e01-8b47-3daf91a0ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.13'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78af398b-7e8a-4098-90b9-c87a283f2663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# !{sys.executable} -m pip install --user --upgrade pydantic==1.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ce7c3e4-0137-4605-9bdb-3d3ceb7f42c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "# till 0.0.350\n",
    "# from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "# from 0.0.354\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a04de-b5ea-4722-bd40-04ae88f4ed91",
   "metadata": {},
   "source": [
    "### Init a HuggingFacePipeline with pipeline_kwargs\n",
    "\n",
    "https://github.com/langchain-ai/langchain/issues/8280#issuecomment-1652085694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ba778fa-e2ad-42bf-bc58-6a1814f24395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id  = \"TheBloke/wizardLM-7B-HF\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\"trust_remote_code\": True},\n",
    "#     pipeline_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"device_map\": \"auto\",\n",
    "#         \"max_new_tokens\": 1200,\n",
    "#         \"temperature\": 0.3,\n",
    "#         \"top_p\": 0.95,\n",
    "#         \"repetition_penalty\": 1.15,\n",
    "#     },\n",
    "# )\n",
    "# print(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "096b51af-4a83-4d5a-913f-5efdf376ac11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthis hack of the partial function doesn't work, since the partial returns a Partial obj and not a Pipeline obj.\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "this hack of the partial function doesn't work, since the partial returns a Partial obj and not a Pipeline obj.\n",
    "\"\"\"\n",
    "# from functools import partial\n",
    "# hg_pipeline = partial(generator, max_new_tokens=80, temperature=0.1, repetition_penalty=1.1, device_map=\"auto\")\n",
    "# llm = HuggingFacePipeline(\n",
    "#     pipeline=hg_pipeline \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9894f6e1-3867-4e08-b2a9-85f89455ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFacePipeline\u001b[0m\n",
      "Params: {'model_id': 'gpt2', 'model_kwargs': None, 'pipeline_kwargs': None}\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "print(llm)\n",
    "print(llm.pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcfdd2fa-8ed4-4f0f-bb97-c4ec02bf8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bug, the HuggingFacePipeine is not getting the param directly\n",
    "# https://github.com/langchain-ai/langchain/issues/8280\n",
    "\n",
    "# this must be set for the generator (HuggingFacePipeline) to work\n",
    "llm.model_id = model_name\n",
    "pipeline_kwargs_config = {\n",
    "    # \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    # \"top_k\": 10, # this param result in trouble with langchain (optional)\n",
    "    # \"num_return_sequences\": 1, # (optional)\n",
    "    # \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    \"device_map\": \"auto\",\n",
    "    \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "    # \"max_length\": None, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": 100, # this is not taken by the model ?\n",
    "    \"temperature\": 0.1,\n",
    "    # \"top_p\": 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.1, # 1.15,\n",
    "}\n",
    "model_kwargs_config = {\n",
    "    \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    \"top_k\": 3, # this param result in trouble with langchain (optional)\n",
    "    \"num_return_sequences\": 1, # (optional)\n",
    "    \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    # \"device_map\": \"auto\",\n",
    "    \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "    # \"max_length\": None, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": 100, # this is not taken by the model ?\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.8, # 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.1, # 1.15,\n",
    "}\n",
    "llm.model_kwargs = model_kwargs_config\n",
    "llm.model_kwargs[\"trust_remote_code\"] = True\n",
    "llm.pipeline_kwargs = pipeline_kwargs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a400b78d-5f21-4ce8-a7ca-4388bdee6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 24.734375 GB\n",
      "Allocated memory : 24.447203 GB\n",
      "Free      memory : 0.287172 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93b51595-0f2f-4076-b52d-94aaabdee9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(llm.pipeline.model.name_or_path)\n",
    "# print(llm.model_id)\n",
    "# print(llm.model_kwargs)\n",
    "# print(llm.pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccd4f4-bb44-4057-b60b-09bd668ff6d8",
   "metadata": {},
   "source": [
    "## Simple local LLM call from langchain API\n",
    "\n",
    "this section tests the call of a local TextGenerationPipeline from langchain API\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2d1f291-f13b-493d-98a8-780321585b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7efea7128f40>, model_id='meta-llama/Llama-2-13b-chat-hf', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'top_p': 0.8, 'repetition_penalty': 1.1, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'repetition_penalty': 1.1})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ad96c05-2201-4a40-a43c-046ee6859a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_func(f: callable):\n",
    "    def inner(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        print(\"=\"*20)\n",
    "        print(f\"walltime: {duration} in secs.\")\n",
    "        print(\"=\"*20)\n",
    "    return inner\n",
    "\n",
    "\n",
    "@time_func\n",
    "def chat_llm(prompt: str):\n",
    "    print(llm(prompt))\n",
    "    # gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89503a14-894e-4b45-94c0-636c896c0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The cafeteria had 23 apples. They used 20, so they have 3 left. Then they bought 6 more, so they have 9 apples now.\n",
      "Q: The weather is 30 degrees and it is sunny. What is the opposite of sunny?\n",
      "A: The opposite of sunny is cloudy.\n",
      "====================\n",
      "walltime: 3.6076815128326416 in secs.\n",
      "====================\n",
      "A: The cafeteria had 23 apples. They used 20, so they have 3 left. Then they bought 6 more, so they have 3 + 6 = 9 apples.\n",
      "Q: If it takes 3 apples to make a pie and they have 9 apples, how many pies can they make?\n",
      "A: They have 9 apples, and it takes 3 apples to make a pie,\n",
      "====================\n",
      "walltime: 4.3148064613342285 in secs.\n",
      "====================\n",
      "A: The cafeteria had 23 apples. They used 20, so they have 3 apples left. They bought 6 more, so they have 9 apples now.\n",
      "Q: What is the answer to the previous question?\n",
      "A: The answer is 9 apples.\n",
      "====================\n",
      "walltime: 2.942945957183838 in secs.\n",
      "====================\n",
      "A: 23 - 20 = 3. They used 20 apples to make lunch. 3 apples are left.\n",
      "Q: How many inches are in 5 feet?\n",
      "A: There are 12 inches in 1 foot. 5 feet is 5 x 12 = 60 inches.\n",
      "====================\n",
      "walltime: 3.241997480392456 in secs.\n",
      "====================\n",
      "A: They started with 23 apples. They used 20, so they have 23 - 20 = 3 apples left. Then they bought 6 more, so they have 3 + 6 = 9 apples.\n",
      "====================\n",
      "walltime: 2.431318998336792 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# %time\n",
    "\"\"\"\n",
    "more time the same question of math, LLM get once wrong\n",
    "\n",
    "Example of wrong answer:\n",
    "A: The cafeteria started with 23 apples. They used 20 to make lunch, leaving 3 apples. \\n\n",
    "Then, they bought 6 more, bringing the total to 23 + 6 = 29 apples. The answer is 29.\n",
    "\n",
    "A: 23 - 20 = 3. They have 3 apples.\n",
    "Q: A bookshelf has 12 books. If they put 4 more books on it, how many books are on the shelf?\n",
    "A: 12 + 4 = 16. There are 16 books on the shelf.\n",
    "\"\"\"\n",
    "\n",
    "# repeat = 10 \n",
    "repeat = 5\n",
    "for _ in range(repeat): # is here a CPU bottleneck? for some reason, if called twice, the model lost the context, will hallucinate.\n",
    "    # chat_llm(prompt=inputs[0])\n",
    "    chat_llm(prompt=get_inputs(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87d496-94e0-4c78-9788-e4771d01bf94",
   "metadata": {},
   "source": [
    "## Sequential Doc Chain\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bef3ff8d-b12b-42a3-bc75-ba07d3793633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "# from langchain.text_splitter import TextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from util.objectstore_utils import S3PdfObjHelper\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31facca0-362a-4431-aaf9-68379e494061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.14\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d473b8ce-88bd-41d2-8ec5-d6a8d1b8ea66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans2en/KK-SCIVIAS\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"scivias-medreports\"\n",
    "file_prefix = \"KK-SCIVIAS\"\n",
    "prefix = f\"{S3PdfObjHelper.DataContract.key_lead}/{file_prefix}\"\n",
    "access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT')\n",
    "# VERIFY = False\n",
    "VERIFY = True\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47ae132f-626c-43fe-9317-ac667cb9ce10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = S3DirectoryLoader(bucket=bucket_name,\n",
    "                           prefix=prefix, \n",
    "                           aws_access_key_id=access_key_id, \n",
    "                           aws_secret_access_key=secret_access_key,\n",
    "                           endpoint_url=s3_endpoint,\n",
    "                           verify = VERIFY,\n",
    "                           use_ssl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5f4078a-0d6c-475f-92b0-948471cbb7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 3 µs, total: 10 µs\n",
      "Wall time: 20.3 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# this is a synchronized call\n",
    "# need to make a custom call to use iterater to load async\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9e0c885-1dbb-4103-81a2-728dcaa36af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DocMetaInfo():\n",
    "    def __init__(self, doc: Document):\n",
    "        self.read_meta(doc)\n",
    "    \n",
    "    \n",
    "    def read_meta(self, doc: Document):\n",
    "        file_content = doc.page_content\n",
    "        self.source = doc.metadata['source']\n",
    "        self.name = self.source.split(\"/\")[-1]\n",
    "        self.token_size = len(file_content.split())\n",
    "        self.character_size = len(file_content)\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"call by print\"\"\"\n",
    "        return f\"source:{self.source}\\nname:{self.name}\\ntokens:{self.token_size}\\ncharacters:{self.character_size}\"\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"convert obj to string, called by jupyter cell\"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "        \n",
    "def print_s3_obj_info(data: List[Document], idx: int, show_content: bool = False):\n",
    "    if (data is not None):\n",
    "        n = len(data)\n",
    "        print(f\"total objects: {n}\")\n",
    "        print(\"=\"*20)\n",
    "        if -n <= idx < n: # in range of list idx\n",
    "            meta_info = DocMetaInfo(data[file_idx])\n",
    "            file_content = data[file_idx].page_content\n",
    "            \n",
    "            print(f\"s3 key     :{meta_info.source}\")\n",
    "            print(f\"obj name   :{meta_info.name}\")\n",
    "            print(f\"token size :{meta_info.token_size}\")\n",
    "            print(f\"char. size :{meta_info.character_size}\")\n",
    "            if show_content:\n",
    "                print(\"-\"*20)\n",
    "                print(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b06520a-948d-4f76-99d0-5931a33660e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_meta_list = [DocMetaInfo(doc) for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6dc2bded-c074-41cd-8fde-ec862f8cab61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
      "name:KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
      "tokens:4568\n",
      "characters:29060\n"
     ]
    }
   ],
   "source": [
    "# enumerate returns a key, element tuple, the x[1] is the DocMetaInfo(doc)\n",
    "# https://stackoverflow.com/questions/16945518/finding-the-index-of-the-value-which-is-the-min-or-max-in-python/16945868#16945868\n",
    "idx_of_max_token, doc_meta = max(enumerate(docs_meta_list), key=lambda x: x[1].token_size)\n",
    "print(doc_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2e99d56-f3c6-4fbf-a0fd-713bcf84f9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
       "name:KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
       "tokens:4568\n",
       "characters:29060"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_meta_list[idx_of_max_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c439bf3-c495-40ab-ba4a-33540e594fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226,\n",
       " source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00182^0054877490^2021-06-28^KIIHORMO.txt\n",
       " name:KK-SCIVIAS-00182^0054877490^2021-06-28^KIIHORMO.txt\n",
       " tokens:516\n",
       " characters:3496)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(enumerate(docs_meta_list), key=lambda x: x[1].token_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75663609-b6b2-44d7-ae6d-752cee0c26fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_idx = 0 # ID 0003 has weight 43.2 kg\n",
    "# file_idx = 1\n",
    "# file_idx = idx_of_max_token\n",
    "show_content = False\n",
    "# show_content = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08ff4c4c-7dab-4088-bd83-61511e65ec94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total objects: 246\n",
      "====================\n",
      "s3 key     :s3://scivias-medreports/trans2en/KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\n",
      "obj name   :KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\n",
      "token size :1030\n",
      "char. size :6977\n"
     ]
    }
   ],
   "source": [
    "print_s3_obj_info(data, file_idx, show_content=show_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbf91c-2933-416e-940e-43b2ea414c5c",
   "metadata": {},
   "source": [
    "### Langchain text splitter\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "176c278e-7bc2-40c8-ac7a-b1fca4980eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = (MAX_POSITION_EMBEDDINGS // 1000) * 1000\n",
    "print(CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b65bcf1f-813f-481a-9e50-d9ebea07483a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = CHUNK_SIZE,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "433b8ab8-07a8-4956-bd12-8d9352e590ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2956\n",
      "2904\n",
      "1113\n"
     ]
    }
   ],
   "source": [
    "# Optional test of RecursiveCharacterTextSplitter on \\n and other chars\n",
    "test_text = data[file_idx].page_content\n",
    "text_split_list = text_splitter.split_text(test_text)\n",
    "print(len(text_split_list))\n",
    "for seg in text_split_list:\n",
    "    print(len(seg))\n",
    "# print(text_split_list[-1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90cbd4-e5fe-48d6-859d-0444a6e66d1b",
   "metadata": {},
   "source": [
    "### Langchain embeddings\n",
    "\n",
    "use sentence-transformers  \n",
    "\n",
    "* all-MiniLM-L12-v2 : 134MB https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 \n",
    "* all-MiniLM-L6-v2 : 90MB https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main\n",
    "\n",
    "Llama2 does not support document embedding by default\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n",
    "\n",
    "HuggingFaceEmbedding embed_documents example\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "In-memory vectorstore need DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "TextEmbeddings in LangChain\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "Sentence-transformers\n",
    "* https://medium.com/@madhur.prashant7/demo-langchain-rag-demo-on-llama-2-7b-embeddings-model-using-chainlit-559c10ce3fbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee02e3af-d40f-4e1f-a9d9-f8fb36572d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1c3086f-9735-4e16-afc4-80d91bafae2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_model_map = {\n",
    "    \"sentence-transformers\": \"sentence-transformers/all-MiniLM-L12-v2\", # 384\n",
    "    \"baai\" : \"BAAI/bge-base-en-v1.5\" # 768 embedding dims\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0aecbe3-434c-4c54-b4c2-2678bc9d4973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embed_model_vendor = \"sentence-transformers\"\n",
    "embed_model_vendor = \"baai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7d63645-9f9e-4bbf-b766-37e5077082bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAAI/bge-base-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "embed_model_name = embed_model_map[embed_model_vendor]\n",
    "print(embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8fb49c77-ef08-4406-953b-864857631989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-base-en-v1.5'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a756bf90-97cd-433d-adf8-891cf631b2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5df4e30-6ded-4be1-a6e3-20af0c674ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {'device': 'cpu'}\n",
    "# model_kwargs = {'device_map': \"auto\",}\n",
    "# encode_kwargs = {'normalize_embeddings': False}\n",
    "encode_kwargs = {'normalize_embeddings': True} # for the cosin similarity search\n",
    "\n",
    "# is downloaded at \"{MODEL_CACHE_DIR}/models/torch/sentence_transformer\" folder\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f36b2c16-5064-4f71-ba9d-77f154154011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768)\n"
     ]
    }
   ],
   "source": [
    "test_docs_list = [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    "\n",
    "def embed_vec_dim(embeddings):\n",
    "    return len(embeddings), len(embeddings[0])\n",
    "\n",
    "def embed_docs_test(model: HuggingFaceEmbeddings, docs_list: list):\n",
    "    embeddings = model.embed_documents(\n",
    "        docs_list\n",
    "    )\n",
    "    return embeddings\n",
    "    len(embeddings), len(embeddings[0])\n",
    "\n",
    "embeddings = embed_docs_test(embed_model, test_docs_list)\n",
    "print(embed_vec_dim(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81c39112-3189-434c-8a57-324afa45d6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "# print(text_split_list)\n",
    "\n",
    "embeddings = embed_docs_test(embed_model, text_split_list)\n",
    "print(embed_vec_dim(embeddings))\n",
    "# print(embeddings[0])\n",
    "# print(embeddings[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e02332-535e-4a00-88b3-3281e69ab372",
   "metadata": {},
   "source": [
    "## Langchain local LLM RAG\n",
    "\n",
    "Langchain Vectorstore and RAG approach differences:\n",
    "* https://github.com/langchain-ai/langchain/issues/5328\n",
    "\n",
    "Langchain RetrievalQA \n",
    "* https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "\n",
    "DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "LLama2 doesn't support Doc Embedding\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9679882f-495e-4262-8d5c-6883997ba053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAG one document\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embed_model,\n",
    "    text_splitter=text_splitter,\n",
    "    ).from_documents([data[file_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db3042e6-eeca-4129-9360-f34568f9b24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RETRIEVER_K = 3 # with two doc, there is not much i don't know\n",
    "retriever = index.vectorstore.as_retriever(search_kwargs={'k': RETRIEVER_K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e9489af-6dd6-4b82-8060-73f2da0ad5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# db = DocArrayInMemorySearch.from_documents(\n",
    "#     [data[file_idx]], embed_model)\n",
    "\n",
    "# retriever = db.as_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a7eec-2d15-4960-84e1-13ae3d40a130",
   "metadata": {},
   "source": [
    "#### Set the custom template\n",
    "\n",
    "Use the object variable, instead of kwargs\n",
    "https://github.com/langchain-ai/langchain/issues/6635#issuecomment-1659343109\n",
    "\n",
    "The reduce_prompt_template can be set\n",
    "```shell\n",
    "qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0bb62d8e-187a-47db-8f07-48833172225a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template = \"\"\"\n",
    "# Given the following extracted parts of a long document and a question, create a final answer.\\n\n",
    "# If you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n\\n\\n\n",
    "# =========\\n\n",
    "# QUESTION: {question}\\n\n",
    "# =========\\n\n",
    "# {summaries}\\n\n",
    "# =========\\n\n",
    "# FINAL ANSWER:\"\"\"\n",
    "\n",
    "# reduce_prompt_template = PromptTemplate(template=template, input_variables=['question', 'summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc66ce02-33e5-49f4-a380-28deb0185fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {context}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n[/INST]')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "# If you don't know the answer to a question, just reply with an empty string '' and please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {context}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "# \n",
    "# chatgpt3_end_token = \"<|im_end|>\"\n",
    "llama_end_token = \"<|end|>\"\n",
    "\n",
    "map_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "CONTEXT:/n/n {context}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "map_prompt_template = PromptTemplate.from_template(map_template)\n",
    "map_prompt_template\n",
    "\n",
    "# Relevant text, if any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbba601d-c36f-45b5-88e8-e3af2a3ecd01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "# Ignore \"I don't know\" or \"not provided\" context text provided, do not use these as answer.\\n\n",
    "# If there are multiple relevant information in the context text provided, chose the majority of the relevant information as answer.\\n\n",
    "# If you know any answer, which is not \"I don't know\" or \"not provided\", chose the relevant information as answer.\\n\n",
    "# If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "\n",
    "# reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "# If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "# If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "#reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Always summarise the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "#If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "#If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "#\n",
    "#CONTEXT:/n/n {summaries}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the summarised answer below and nothing else.\n",
    "#Summarised answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "\n",
    "reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Always summarise the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "If you don't know the answer to a question, please don't share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the summarised answer below and nothing else.\n",
    "Summarised answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "reduce_prompt_template = PromptTemplate.from_template(reduce_template)\n",
    "reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "340b2cd7-7d39-4d3d-945f-c05d2f9fdaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context_str', 'question'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nContext:/n/n {context_str}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n[/INST]')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "#If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "#\n",
    "#Context:/n/n {context_str}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the helpful answer below and nothing else.\n",
    "#Helpful answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "# <|end|> for llama\n",
    "refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "Context:/n/n {context_str}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "init_prompt_template = PromptTemplate.from_template(refine_init_template)\n",
    "init_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3459329d-ad92-4c73-8084-ecfab6b1d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "# \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "\n",
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"stuff\"\n",
    "# chain_type = \"refine\" \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    )\n",
    "# set the prompt template manually\n",
    "# use the original prompttemplate to do the summary, the current custom template doesn't have the one-short summary example, but just the right format.\n",
    "# qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b3c75aa-1fbf-4d2d-a601-a7d77976059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chain_type == \"map_reduce\":\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "    # set the token max from 3000 to 4000\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS\n",
    "    \n",
    "    \n",
    "if chain_type == \"refine\":\n",
    "    # pass\n",
    "    qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "    # qa_chain.combine_documents_chain.refine_llm_chain.token_max = MAX_POSITION_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7d997-f0e4-4f60-837c-88c45784ce57",
   "metadata": {},
   "source": [
    "### Set token max or max token for the llm\n",
    "* https://github.com/langchain-ai/langchain/issues/434#issuecomment-1440312002\n",
    "* https://github.com/langchain-ai/langchain/issues/9341#issuecomment-1681306494\n",
    "* https://github.com/langchain-ai/langchain/issues/9341#issuecomment-1681306494"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc9e36-7a4f-42b5-abf0-c086aa0b3b91",
   "metadata": {},
   "source": [
    "## Set debug mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f775ca5a-1974-4837-8821-b556266cee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set DEBUG to false to remove all the llm answer outputs\n",
    "# DEBUG=True\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e2db375-471a-413c-9e4a-c0245071d4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {context}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7efea7128f40>, model_id='meta-llama/Llama-2-13b-chat-hf', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'top_p': 0.8, 'repetition_penalty': 1.1, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'repetition_penalty': 1.1})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7efea7128f40>, model_id='meta-llama/Llama-2-13b-chat-hf', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'top_p': 0.8, 'repetition_penalty': 1.1, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'repetition_penalty': 1.1})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7effc574ba90>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db64fcd7-e27d-45b6-877d-a3d2cb6b0455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the name of the patient? (Remember to include 'The name of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69cb1d99-648d-4cde-a535-a9f194c6c515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cabfb9f4-be94-4887-9905-9f844fe89511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23097cc6-475d-4a60-a581-51fe163c0dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PromptParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8de91a84-088f-4e8b-8df1-1eeaa4678ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, TransformChain\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following medical report, extract the following information:\n",
    "\n",
    "# patient_name: Extract only the name of the patient, \\\n",
    "# do not extract any name that is not patient.\n",
    "# Answer with the patient name if you find it, \"\" if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# report: {text}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\n",
    "\n",
    "# patient_name: Extract only the name of the patient, \\\n",
    "# do not extract any name that is not patient.\n",
    "# Answer with the patient name if you find it, \"\" if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# text: {text}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\\\n",
    "\n",
    "# patient_name: Extract the name of the patient. \\\n",
    "# Answer only one name, answer with the name if you find it, answer None if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# text:\\\n",
    "# {text}\n",
    "\n",
    "# {format_instructions}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\n",
    "\n",
    "# patient_name: the name of the patient, \\\n",
    "# Answer with the patient name, \\\"\\\" if not or unknown.\n",
    "\n",
    "# {format_instructions}\n",
    "\n",
    "# text:\\\n",
    "# {text}\n",
    "# \"\"\"\n",
    "\n",
    "query_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "CONTEXT:/n/n {text}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "{format_instructions}/n\n",
    "\n",
    "[/INST]\"\"\"\n",
    "\n",
    "\n",
    "# name_query_template = \"\"\"\n",
    "# retrieve one: patient name, from the following text.\\n format response as following \\{\"patient_name\": patient name\\}\\n Text: {text}\"\"\"\n",
    "\n",
    "# name_schema = ResponseSchema(name=\"patient_name\", description=\"patient name, \\\n",
    "# Answer with the patient name, \\\"\\\" if not or unknown.\")\n",
    "name_schema = ResponseSchema(name=\"patient_name\", description=\"patient name\")\n",
    "\n",
    "response_schema = [name_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c31875-3ade-42ce-a171-670c2c0eaf62",
   "metadata": {},
   "source": [
    "### LLama2 prompt style\n",
    "* https://colab.research.google.com/drive/1hRjxdj53MrL0cv5LOn1l0VetFC98JvGR?usp=sharing#scrollTo=IrVIuygNuBVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f6cca58-88a5-42b1-bb95-fa64748c84b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Default LLaMA-2 prompt style\n",
    "# B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "# B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "# def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "#     SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "#     prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "#     return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ad27b24f-1a8c-4973-9229-45267e05fdae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
    "\n",
    "# instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "# Question: {question}\"\"\"\n",
    "# get_prompt(instruction, sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4cd23b0f-150f-40ad-9319-7032da913d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"patient_name\": string  // patient name\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "12321b68-27e9-4789-9a0b-15bd72ef2a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt_template = PromptTemplate.from_template(get_prompt(instruction, sys_prompt))\n",
    "# prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "47edb902-0751-4905-b451-6ee3d6ce8436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['format_instructions', 'question', 'text'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {text}/n/n/n\\n\\nQuestion: {question}/n/n\\n{format_instructions}/n\\n\\n[/INST]\")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_template = ChatPromptTemplate.from_template(name_query_template) # ChatPromptTemplate create Human and Output in the text\n",
    "name_question=\"retrieve one: patient name\"\n",
    "prompt_template = PromptTemplate.from_template(query_template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f4dbaad-894c-4ac7-bd7c-72d03e63211e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = response['result'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f137f7fb-e68b-4b12-8ac6-c9f74387231b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# messages = prompt_template.format_prompt(text=input_text, format_instructions=format_instructions)\n",
    "# messages = prompt_template.format_messages(text=input_text, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db23d37d-4aae-4edc-a6ec-0a11f6102dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['format_instructions', 'question', 'text'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {text}/n/n/n\\n\\nQuestion: {question}/n/n\\n{format_instructions}/n\\n\\n[/INST]\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7efea7128f40>, model_id='meta-llama/Llama-2-13b-chat-hf', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'top_p': 0.8, 'repetition_penalty': 1.1, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'repetition_penalty': 1.1}))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51c67d20-ce3b-4256-bb4f-0af283a8089d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True \n",
    "# parser_response = chain.run(context=input_text, question=question, temperature=0.0)\n",
    "parser_response = chain.run(text=input_text, format_instructions=format_instructions, question=name_question, temperature=0.0)\n",
    "# parser_response = chain.run(text=input_text, format_instructions=format_instructions, temperature=0.0)\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ac56dda-ad3e-4b8d-bec2-491c6410af5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_response_dict(parser_response: str, output_parser, verbose: bool=False) -> dict:\n",
    "    # https://stackoverflow.com/questions/24667065/python-regex-difference-between-and/24667099#24667099\n",
    "    # https://stackoverflow.com/questions/33312175/matching-any-character-including-newlines-in-a-python-regex-subexpression-not-g/33312193#33312193\n",
    "    # (.+) is greedy, (.+?) stops at the first match\n",
    "    try:\n",
    "        post_proccessed_response = re.search(r\"```[\\s\\S]+```\", parser_response).group(0)\n",
    "        if verbose:\n",
    "            print(post_proccessed_response)\n",
    "        output_dict = output_parser.parse(post_proccessed_response)\n",
    "        key = output_parser.response_schemas[0].name\n",
    "        if output_dict.get(key) is None:\n",
    "            output_dict[key] = \"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        output_dict = {}\n",
    "    return output_dict\n",
    "\n",
    "output_parser\n",
    "\n",
    "patient_name = parse_response_dict(parser_response, output_parser).get(\"patient_name\", \"\").strip() \n",
    "if DEBUG:\n",
    "    print(f\"str response: {parser_response}\")\n",
    "    print(f\"patient_name is: {patient_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286bd31-8bb2-4e65-8a2b-23be2ac5f3a8",
   "metadata": {},
   "source": [
    "### Age question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dbc5db03-faaa-4870-9a66-6e617a629902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_name=f\"{patient_name}\" if patient_name is not None else \"\"\n",
    "# query = f\"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\"\n",
    "query = f\"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\"\n",
    "if DEBUG:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bcd9a56f-1c2e-4fb9-ac58-8db56a7a0861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"stuff\"\n",
    "# chain_type = \"refine\" \n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3bd801cb-4e39-424e-8310-9b013f740a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if chain_type == \"map_reduce\":\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "    # set the token max from 3000 to 4000\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS\n",
    "    \n",
    "    \n",
    "if chain_type == \"refine\":\n",
    "    # pass\n",
    "    qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "    # qa_chain.combine_documents_chain.refine_llm_chain.token_max = MAX_POSITION_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b52cd2df-08f5-45bc-9f5c-9f94926fe4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {context}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7efea7128f40>, model_id='meta-llama/Llama-2-13b-chat-hf', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'top_p': 0.8, 'repetition_penalty': 1.1, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'repetition_penalty': 1.1})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7efea7128f40>, model_id='meta-llama/Llama-2-13b-chat-hf', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'top_p': 0.8, 'repetition_penalty': 1.1, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 100, 'temperature': 0.1, 'repetition_penalty': 1.1})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7effc574ba90>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9713d2c7-fcfe-4934-b0c1-b1ae9e7df4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qa_chain.combine_documents_chain.initial_llm_chain.prompt\n",
    "# qa_chain.combine_documents_chain.refine_llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3bbe2371-ed99-4cc0-9875-29c2d3bd5c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fe0b73af-3d35-4426-a272-1d63a8afdb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e7ef2daa-235d-4fd8-afee-180e3446812a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the string type\n",
    "# age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\", type=\"int\")\n",
    "age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\")\n",
    "# age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\", type=\"int\")\n",
    "\n",
    "# response_schema = [age_schema]\n",
    "age_output_parser = StructuredOutputParser.from_response_schemas([age_schema])\n",
    "# age_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6584cd99-239b-476f-8932-05235dfeab48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"patient_age\": string  // patient age\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "age_question=\"retrieve one: patient age\"\n",
    "format_instructions = age_output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aea79c9e-923e-43b7-bdaa-abc17e2aadcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7e21b869-b9fe-47bf-b8a2-58f20baf5b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "39761501-39b9-4cd2-81df-27be8c9bfd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True \n",
    "parser_response = chain.run(text=input_text, format_instructions=format_instructions, question=age_question, temperature=0.0)\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a39e5a67-203f-47b1-93e7-d0e00d3ceac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(parser_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1c11d626-8be5-405f-8421-9b8ec30b3ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n"
     ]
    }
   ],
   "source": [
    "patient_age_obj = parse_response_dict(parser_response, age_output_parser, DEBUG).get(\"patient_age\", \"\")\n",
    "# print(patient_age_str)\n",
    "try:\n",
    "    if isinstance(patient_age_obj, str):\n",
    "        patient_age_obj = patient_age_obj.strip()\n",
    "        patient_age = int(patient_age_obj)\n",
    "    if isinstance(patient_age_obj, int):\n",
    "        patient_age = patient_age_obj\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    patient_age = -1\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"str response: {parser_response}\")\n",
    "    print(f\"patient_age is: {patient_age}\")\n",
    "    print(f\"pateint_age has type: {type(patient_age)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "438568b1-f46f-4bb4-8ff2-1aa89b8015c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(input_text)\n",
    "    print(parser_response)\n",
    "    print(f\"{patient_name} is {patient_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "56d01aff-1853-4e8c-a7a8-2edbb2e379cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map_prompt_template = PromptTemplate(template=\"\"\"\n",
    "# Use the following portion of a long document to see if any of the text is relevant to answer the question. \\n\n",
    "# Return any relevant text verbatim.\\n\n",
    "# {context}\\n\n",
    "# Question: {question}\\n\n",
    "# Relevant text, if any:\"\"\",\n",
    "# input_variables=['context', 'question'])\n",
    "# qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cc180169-0db6-44e5-9d32-2d4ab4c1a3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stuff_llm_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n\n",
    "# {context}\\n\\n\n",
    "# Question: {question}\\nHelpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "557a0fff-560f-497e-ab1c-2e6b01945acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30dc0290-7669-4132-b40f-806176b7f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the weight of the patient in kg? (Remember to include 'The weight of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c55acbd2-237a-40be-92fb-de2dedfdd9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     langchain.debug = True\n",
    "# response = qa_chain({\"query\": query})\n",
    "# #response = qa_chain({\"query\": query})\n",
    "# if DEBUG:\n",
    "#     langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2113a349-9774-4ba7-86fe-9cd9b66f5b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     print(f\"Response: {response['result']}\")\n",
    "#     print('-'*20)\n",
    "#     print(data[file_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98068bfa-b127-4653-8176-4e0e0bbc378a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a2436c90-e04b-4a2f-b1aa-894d52a12c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response = index.query_with_sources(query, llm=llm, retriever_kwargs={\"chain_type\":\"map_reduce\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74bf5f-fdad-4843-96c3-95844d7d91fa",
   "metadata": {},
   "source": [
    "### (optional) Additional Read\n",
    "\n",
    "GPT4All\n",
    "* https://python.langchain.com/docs/integrations/llms/gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c03817-da5b-445d-b3cc-ec7cd8a29cf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9fd27939-7b6e-4a5c-884f-c4813825c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861587f9-f41e-4ce3-903b-ec50b17f4cce",
   "metadata": {},
   "source": [
    "#### zero shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4b5316fa-82d8-4f47-98fa-9506916a1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name\n",
    "input=f\"Can you tell me the name of the patient from the folowing doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ddd6b804-67bd-4c54-a7c2-0e5ef14aa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input)\n",
    "# 6810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "780a0f12-7d99-4aba-9c8a-5fb1e28f456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3c299015-c5b7-425b-8a79-ba155491c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age\n",
    "input=f\"Can you tell me the age of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9fcf966a-76c1-4e7d-8a6d-0b356b2b86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8d4d472f-80db-4d73-ac73-f673bc5fe279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnosis\n",
    "input=f\"Can you tell me the diagnosis of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51d7927d-9cba-4cbf-a05c-4fcaf886c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa11f-0eb2-4dac-aa70-194cfe8e577d",
   "metadata": {},
   "source": [
    "#### Chain-of-thoughts prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name prompt\n",
    "input = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {context}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7881162a-6514-4f31-a64c-60718b2f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6e61eea4-3731-4ba5-8c1b-e392cc6fe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "input = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{context}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "# print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ca31ec3d-1912-4a07-aab0-c84151783112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "#len(input)\n",
    "# > 6913 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "80fbeb59-1fe4-4ab0-b004-05b2fe2d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3a401151-af8b-46f4-8ef2-0177ae075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose prompt\n",
    "input=f\"Context:\\nPatient: Fried is a 34-year-old patient, Diagnoses: Influenza (J09.X2) \\nQuestion:\\nWhat diagnoses has the patient? \\nAnswer:\\nFried is a patient, 34 year-old, has diagnoses Influenza (J09.X2). The answers is Influenza (J09.X2)\\nContext:\\n{context}\\nQuestion:\\nWhat diagnoses has the patient?\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "efd94444-a458-40dd-b95d-2be26f9ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "374b6e3c-3d58-4cf6-b834-1d066519e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "024d74a3-4ef3-4379-9075-a3944812244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
