{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4946d3e4-c86a-497c-a0e4-21c89b93799d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@created: 24.11.2023\\\n",
    "@updated: 28.11.2023\\\n",
    "@version: 2\n",
    "\n",
    "This notebook comprises of examples to use transformer, pytorch, llama2, langchain to achive entity extraction with engineered prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04aee56a-508c-4010-b447-f6ab9db29e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pydantic_core is from 2.xx version doesn't work with docarray 0.39.1\n",
    "# !{sys.executable} -m pip uninstall pydantic_core -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install --user --upgrade kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac05d7-4f2e-4fc0-99f8-bca2ffffe3cc",
   "metadata": {},
   "source": [
    "## Use the cuda 118 and torch 2.1.0 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a925dce-3bdf-4a58-9661-7715a977c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48662e9c-04da-455f-a80d-bd8136ee5f2e",
   "metadata": {},
   "source": [
    "## Additional technical informaiton\n",
    "#### Useful installation for KF notebook 1.7.0 cu111 drivers\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade transformers==4.31.0\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==1.10.2+cu111 fastai==2.7.12 fastcore==1.5.29 fastdownload==0.0.7 torchvision==0.11.3+cu111 --extra-index-url https://download.pytorch.org/whl/cu111\n",
    "#!{sys.executable} -m pip install --user --upgrade accelerate==0.20.3\n",
    "```\n",
    "cuda118\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==2.0.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "`xformers==0.0.21` need `torch==2.0.1`\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade xformers==0.0.21 torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "```\n",
    "\n",
    "show js loading with ipywidgets\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0 comm==0.1.4 jupyterlab-widgets==3.0.8 widgetsnbextension==4.0.8\n",
    "```\n",
    "\n",
    "uninstall\n",
    "```shell\n",
    "#!{sys.executable} -m pip uninstall accelerator transformers xformers torch -y \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "#### Basics of GPU\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "#### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "#### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2594fcbd-b0a4-42e0-8ce8-e524423d7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from util.accelerator_utils import AcceleratorStatus, AcceleratorHelper\n",
    "\n",
    "# data volume mounted in kubeflow notebook\n",
    "MODEL_ROOT=\"/home/jovyan/llm-models\"\n",
    "MODEL_SUB_PATH = \"core-kind/yinwang\"\n",
    "# the cache dir for huggingface models\n",
    "MODEL_CACHE_DIR = f\"{MODEL_ROOT}/{MODEL_SUB_PATH}\"\n",
    "\n",
    "gpu_status = AcceleratorStatus()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a268056f-65d5-4e2c-a73a-75570990bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_helper = AcceleratorHelper()\n",
    "# dynamically fetch attached accelerator devices\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd3f3cf-f74e-4377-964d-a5c0c1b667bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-0efc9f06-6dca-5886-98af-0273ca7fde51\n",
      "/home/jovyan/llm-models/core-kind/yinwang/models\n"
     ]
    }
   ],
   "source": [
    "# init all the cuda torch env and model download cache directory\n",
    "gpu_helper.init_cuda_torch(UUIDs, MODEL_CACHE_DIR)\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(os.environ[\"XDG_CACHE_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "model_map = {\n",
    "        \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"70B\" : \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        # \"70B\" : \"meta-llama/Llama-2-70b-hf\" \n",
    "}\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "# from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the huggingface hub token\n",
    "\"\"\"\n",
    "token_sub_path = \".cache/huggingface/token\"\n",
    "token_file_path = f\"{MODEL_CACHE_DIR}/{token_sub_path}\"\n",
    "# stripe the leading and tailing EOL chars\n",
    "# https://stackoverflow.com/questions/275018/how-can-i-remove-a-trailing-newline/275025#275025\n",
    "with open (token_file_path, \"r\") as file:\n",
    "    # file read add a new line to the token, remove it.\n",
    "    # token = file.read().replace('\\n', '')    \n",
    "    token = file.read().strip()\n",
    "\n",
    "# print the raw string to see if there is new line in the token\n",
    "# print(r'{}'.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "# model_type = \"13B\"\n",
    "model_type = \"7B\"\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    token=token, #transformer>=4.32.1\n",
    "    device='cpu',\n",
    "    # device_map=\"auto\", # put to GPU\n",
    "    # use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc864b9a-eac7-4335-bbdc-a711b8b664b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f43af-d423-436d-95e2-b45465ed5be2",
   "metadata": {},
   "source": [
    "#### Testing tokenizer\n",
    "https://huggingface.co/docs/tokenizers/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "767706d4-ffd0-4e73-aec2-e18de0eb617c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55be8858-a5ce-4467-a82e-cc1dc8b8a8db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 660, 29901, 14159, 756, 29871, 29941, 22556, 26563, 29889, 940, 1321, 952, 29871, 29906, 901, 508, 29879, 310, 22556, 26563, 29889, 7806, 508, 756, 29871, 29946, 22556, 26563, 29889, 1128, 1784, 22556, 26563, 947, 540, 505, 1286, 29973, 13, 29909, 29901, 14159, 4687, 411, 29871, 29941, 26563, 29889, 29871, 29906, 508, 29879, 310, 29871, 29946, 22556, 26563, 1269, 338, 29871, 29947, 22556, 26563, 29889, 29871, 29941, 718, 29871, 29947, 353, 29871, 29896, 29896, 29889, 450, 1234, 338, 29871, 29896, 29896, 29889, 13, 29984, 29901, 450, 274, 2142, 1308, 423, 750, 29871, 29906, 29941, 623, 793, 29889, 960, 896, 1304, 29871, 29906, 29900, 304, 1207, 301, 3322, 322, 18093, 29871, 29953, 901, 29892, 920, 1784, 623, 793, 437, 896, 505, 29973, 13]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f725e85f-ceab-4c4d-8efe-03fbf089a2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd687825-b00c-4909-95f9-373e8f1bdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "# not loading to the GPU with accelerator\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b07f9052-1ad4-41ff-af51-3bd4394e4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # will call the AutoModelForCausalLM automatically\n",
    "# generator = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     token=token, #transformer>=4.32.1\n",
    "#     #use_auth_token=token, #transformer==4.31.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c182261-d89f-4826-9b67-5c43e89d1ff5",
   "metadata": {},
   "source": [
    "### Inference with transformers pipeline\n",
    "\n",
    "Reference:\n",
    "* https://huggingface.co/docs/transformers/pipeline_tutorial\n",
    "\n",
    "Note:\n",
    "* batch is not activated by default, batch is not necessary faster for `transformers.pipeline`\n",
    "* the max_new_tokens set in the pipeline initialization works with langchain.llms.HuggingFacePipeline, but not as a param for the TextGenerationPipeline \n",
    "\n",
    "max_new_tokens https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f6724600a0408fb4f323ac3bc28177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# in Transformer 4.32.1 need to use \"token\" parameter\n",
    "# in Transformer 4.30.x need to use \"use_auth_token\" parameter\n",
    "# with torch.no_grad():\n",
    "generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    # model=model,\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer, # optional\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_length=None, # remove the total length of the generated response\n",
    "    max_new_tokens=100, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "    token=token, #transformer>=4.32.1\n",
    "    #use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77b83ae3-e8b1-4be6-b2e3-7819a9b5cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24bfaf57-d2e0-464d-bd8e-16eb98a6c4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 12.615234 GB\n",
      "Allocated memory : 12.613792 GB\n",
      "Free      memory : 0.001442 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c3232-c9e7-420a-8769-26dfdfb96b1b",
   "metadata": {},
   "source": [
    "## Passing temparature to the generator for each prompt\n",
    "\n",
    "https://discuss.huggingface.co/t/how-to-set-generation-parameters-for-transformers-pipeline/48837\n",
    "\n",
    "LLama2 chat agent\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-70b-chat-agent.ipynb\n",
    "\n",
    "max_length and max_new_tokens only one need to be set\n",
    "https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f775ca5a-1974-4837-8821-b556266cee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set DEBUG to false to remove all the llm answer outputs\n",
    "# DEBUG=True\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_answer(answer: list)-> None:\n",
    "#     if DEBUG:\n",
    "#         print(\"-\"*10)\n",
    "#         print(answer[0])\n",
    "#         print(\"-\"*10)\n",
    "#         print(answer[0].split(\"\\n\")[-1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d64be04-c528-426b-91f1-47b71996d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# def free_memory_gen(\n",
    "#     generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "#     tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     def local():\n",
    "#         l_generator = generator\n",
    "#         l_tokenizer = tokenizer\n",
    "#         #l_generator.cpu()\n",
    "#         #l_tokenizer.cpu()\n",
    "#         # model.cpu()\n",
    "        \n",
    "#         del l_tokenizer, l_generator\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         #for device_idx in range(torch.cuda.device_count()):\n",
    "#         #    print(device_idx)\n",
    "#         #    device = torch.device(f\"cuda:{device_idx}\")\n",
    "#         #    device.reset()\n",
    "#     return local    \n",
    "\n",
    "# free_memory = free_memory_gen(generator, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bfad65b-f23b-4e99-b724-57dd530b99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chain of thoughts prompting\n",
    "\n",
    "# testing prompt\n",
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: First, the cafeteria had 23 apples. Then, they used 20 to make lunch, leaving 3 apples. Finally, they bought 6 more, so they have 3 + 6 = 9 apples. The answer is 9.\n",
      "--------------------\n",
      "walltime: 3.6301612854003906 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 12.955078 GB\n",
      "Allocated memory : 12.621727 GB\n",
      "Free      memory : 0.333351 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])\n",
    "    \n",
    "# note: the expected answer is 9    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a433d07-366b-437f-a164-d7a84947b65e",
   "metadata": {},
   "source": [
    "## Huggingface with Local LLM\n",
    "\n",
    "* https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
    "\n",
    "HuggingFacePipeline from langchain need pydantic>=1.10.13\n",
    "\n",
    "```shell\n",
    "import pydantic\n",
    "print(pydantic.__version__)\n",
    "```\n",
    "* https://stackoverflow.com/questions/76313592/import-langchain-error-typeerror-issubclass-arg-1-must-be-a-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec0ecea0-3c65-4fd4-81c1-9847a6b2e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFacePipeline version 0.0.313 need pydantic >= 1.10.13\n",
    "# HuggingFacePipeline works in version 0.0.312 with pydantic <= 1.10.2\n",
    "\n",
    "# !{sys.executable} -m pip install --user --upgrade langchain==0.0.341\n",
    "# !{sys.executable} -m pip install --user --upgrade langchain==0.0.312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afb42d96-a471-4e01-8b47-3daf91a0ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.13'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78af398b-7e8a-4098-90b9-c87a283f2663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# !{sys.executable} -m pip install --user --upgrade pydantic==1.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ce7c3e4-0137-4605-9bdb-3d3ceb7f42c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.349\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a04de-b5ea-4722-bd40-04ae88f4ed91",
   "metadata": {},
   "source": [
    "### Init a HuggingFacePipeline with pipeline_kwargs\n",
    "\n",
    "https://github.com/langchain-ai/langchain/issues/8280#issuecomment-1652085694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ba778fa-e2ad-42bf-bc58-6a1814f24395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id  = \"TheBloke/wizardLM-7B-HF\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\"trust_remote_code\": True},\n",
    "#     pipeline_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"device_map\": \"auto\",\n",
    "#         \"max_new_tokens\": 1200,\n",
    "#         \"temperature\": 0.3,\n",
    "#         \"top_p\": 0.95,\n",
    "#         \"repetition_penalty\": 1.15,\n",
    "#     },\n",
    "# )\n",
    "# print(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "096b51af-4a83-4d5a-913f-5efdf376ac11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthis hack of the partial function doesn't work, since the partial returns a Partial obj and not a Pipeline obj.\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "this hack of the partial function doesn't work, since the partial returns a Partial obj and not a Pipeline obj.\n",
    "\"\"\"\n",
    "# from functools import partial\n",
    "# hg_pipeline = partial(generator, max_new_tokens=80, temperature=0.1, repetition_penalty=1.1, device_map=\"auto\")\n",
    "# llm = HuggingFacePipeline(\n",
    "#     pipeline=hg_pipeline \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9894f6e1-3867-4e08-b2a9-85f89455ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFacePipeline\u001b[0m\n",
      "Params: {'model_id': 'gpt2', 'model_kwargs': None, 'pipeline_kwargs': None}\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "print(llm)\n",
    "print(llm.pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcfdd2fa-8ed4-4f0f-bb97-c4ec02bf8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bug, the HuggingFacePipeine is not getting the param directly\n",
    "# https://github.com/langchain-ai/langchain/issues/8280\n",
    "\n",
    "# this must be set for the generator (HuggingFacePipeline) to work\n",
    "llm.model_id = model_name\n",
    "pipeline_kwargs_config = {\n",
    "    # \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    # \"top_k\": 10, # this param result in trouble with langchain (optional)\n",
    "    # \"num_return_sequences\": 1, # (optional)\n",
    "    # \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    \"device_map\": \"auto\",\n",
    "    \"max_length\": None, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": 100, # this is not taken by the model ?\n",
    "    \"temperature\": 0.1,\n",
    "    # \"top_p\": 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.1, # 1.15,\n",
    "}\n",
    "model_kwargs_config = {\n",
    "    \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    \"top_k\": 5, # this param result in trouble with langchain (optional)\n",
    "    \"num_return_sequences\": 1, # (optional)\n",
    "    \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    # \"device_map\": \"auto\",\n",
    "    \"max_length\": None, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": 100, # this is not taken by the model ?\n",
    "    \"temperature\": 0.1,\n",
    "    # \"top_p\": 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.1, # 1.15,\n",
    "}\n",
    "llm.model_kwargs = model_kwargs_config\n",
    "llm.model_kwargs[\"trust_remote_code\"] = True\n",
    "llm.pipeline_kwargs = pipeline_kwargs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a400b78d-5f21-4ce8-a7ca-4388bdee6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 2g.20gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 28\n",
      "Physical  memory : 19.500000 GB\n",
      "Reserved  memory : 12.955078 GB\n",
      "Allocated memory : 12.621727 GB\n",
      "Free      memory : 0.333351 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93b51595-0f2f-4076-b52d-94aaabdee9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(llm.pipeline.model.name_or_path)\n",
    "# print(llm.model_id)\n",
    "# print(llm.model_kwargs)\n",
    "# print(llm.pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccd4f4-bb44-4057-b60b-09bd668ff6d8",
   "metadata": {},
   "source": [
    "## Simple local LLM call from langchain API\n",
    "\n",
    "this section tests the call of a local TextGenerationPipeline from langchain API\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2d1f291-f13b-493d-98a8-780321585b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fbfe7fb1c10>, model_id='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'do_sample': True, 'top_k': 5, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': None, 'max_new_tokens': 100, 'temperature': 0.1, 'repetition_penalty': 1.1, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': None, 'max_new_tokens': 100, 'temperature': 0.1, 'repetition_penalty': 1.1})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ad96c05-2201-4a40-a43c-046ee6859a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_func(f: callable):\n",
    "    def inner(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        print(\"=\"*20)\n",
    "        print(f\"walltime: {duration} in secs.\")\n",
    "        print(\"=\"*20)\n",
    "    return inner\n",
    "\n",
    "\n",
    "@time_func\n",
    "def chat_llm(prompt: str):\n",
    "    print(llm(prompt))\n",
    "    # gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89503a14-894e-4b45-94c0-636c896c0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The cafeteria started with 23 apples. They used 20 to make lunch, leaving 3 apples. Then, they bought 6 more, so now they have 3 + 6 = 9 apples. The answer is 9.\n",
      "Q: If a book costs $15 and a pencil costs $0.50, how much will 3 books and 5 pencils cost in total?\n",
      "A\n",
      "====================\n",
      "walltime: 4.204752445220947 in secs.\n",
      "====================\n",
      "A: The cafeteria had 23 apples to start. They used 20 to make lunch, leaving 3 apples. They then bought 6 more, bringing the total to 9 apples. The answer is 9.\n",
      "Q: Sarah has 15 marbles in her pocket. If she gives 3 to her friend, how many marbles does she have left?\n",
      "A: Sarah had 15 marbles to start. She\n",
      "====================\n",
      "walltime: 4.158351182937622 in secs.\n",
      "====================\n",
      "A: The cafeteria started with 23 apples. They used 20 to make lunch, so they have 23 - 20 = 3 apples left. Then they bought 6 more, so they have 3 + 6 = 9 apples. The answer is 9.\n",
      "====================\n",
      "walltime: 2.9075541496276855 in secs.\n",
      "====================\n",
      "A: The cafeteria had 23 apples to start with. They used 20 apples to make lunch, leaving 23 - 20 = 3 apples. Then they bought 6 more apples, so now they have 3 + 6 = 9 apples. The answer is 9.\n",
      "Q: Sarah has 5 pencils in her pencil case. She gives 2 to her friend. How many\n",
      "====================\n",
      "walltime: 4.156978607177734 in secs.\n",
      "====================\n",
      "A: The cafeteria had 23 apples to start with. They used 20 to make lunch, so they have 23 - 20 = 3 apples left. Then, they bought 6 more, so they have 3 + 6 = 9 apples now. The answer is 9.\n",
      "====================\n",
      "walltime: 3.075347900390625 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# %time\n",
    "\"\"\"\n",
    "more time the same question of math, LLM get once wrong\n",
    "\n",
    "Example of wrong answer:\n",
    "A: The cafeteria started with 23 apples. They used 20 to make lunch, leaving 3 apples. \\n\n",
    "Then, they bought 6 more, bringing the total to 23 + 6 = 29 apples. The answer is 29.\n",
    "\"\"\"\n",
    "\n",
    "# repeat = 10 \n",
    "repeat = 5\n",
    "for _ in range(repeat): # is here a CPU bottleneck? for some reason, if called twice, the model lost the context, will hallucinate.\n",
    "    chat_llm(prompt=inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87d496-94e0-4c78-9788-e4771d01bf94",
   "metadata": {},
   "source": [
    "## Sequential Doc Chain\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bef3ff8d-b12b-42a3-bc75-ba07d3793633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "# from langchain.text_splitter import TextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from util.objectstore_utils import S3PdfObjHelper\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31facca0-362a-4431-aaf9-68379e494061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29.0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d473b8ce-88bd-41d2-8ec5-d6a8d1b8ea66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans2en/KK-SCIVIAS\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"scivias-medreports\"\n",
    "file_prefix = \"KK-SCIVIAS\"\n",
    "prefix = f\"{S3PdfObjHelper.DataContract.key_lead}/{file_prefix}\"\n",
    "access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT')\n",
    "# VERIFY = False\n",
    "VERIFY = True\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47ae132f-626c-43fe-9317-ac667cb9ce10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = S3DirectoryLoader(bucket=bucket_name,\n",
    "                           prefix=prefix, \n",
    "                           aws_access_key_id=access_key_id, \n",
    "                           aws_secret_access_key=secret_access_key,\n",
    "                           endpoint_url=s3_endpoint,\n",
    "                           verify = VERIFY,\n",
    "                           use_ssl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5f4078a-0d6c-475f-92b0-948471cbb7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 3 µs, total: 11 µs\n",
      "Wall time: 20.3 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# this is a synchronized call\n",
    "# need to make a custom call to use iterater to load async\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9e0c885-1dbb-4103-81a2-728dcaa36af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DocMetaInfo():\n",
    "    def __init__(self, doc: Document):\n",
    "        self.read_meta(doc)\n",
    "    \n",
    "    \n",
    "    def read_meta(self, doc: Document):\n",
    "        file_content = doc.page_content\n",
    "        self.source = doc.metadata['source']\n",
    "        self.name = self.source.split(\"/\")[-1]\n",
    "        self.token_size = len(file_content.split())\n",
    "        self.character_size = len(file_content)\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"call by print\"\"\"\n",
    "        return f\"source:{self.source}\\nname:{self.name}\\ntokens:{self.token_size}\\ncharacters:{self.character_size}\"\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"convert obj to string, called by jupyter cell\"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "        \n",
    "def print_s3_obj_info(data: List[Document], idx: int, show_content: bool = False):\n",
    "    if (data is not None):\n",
    "        n = len(data)\n",
    "        print(f\"total objects: {n}\")\n",
    "        print(\"=\"*20)\n",
    "        if -n <= idx < n: # in range of list idx\n",
    "            meta_info = DocMetaInfo(data[file_idx])\n",
    "            file_content = data[file_idx].page_content\n",
    "            \n",
    "            print(f\"s3 key     :{meta_info.source}\")\n",
    "            print(f\"obj name   :{meta_info.name}\")\n",
    "            print(f\"token size :{meta_info.token_size}\")\n",
    "            print(f\"char. size :{meta_info.character_size}\")\n",
    "            if show_content:\n",
    "                print(\"-\"*20)\n",
    "                print(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b06520a-948d-4f76-99d0-5931a33660e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_meta_list = [DocMetaInfo(doc) for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6dc2bded-c074-41cd-8fde-ec862f8cab61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
      "name:KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
      "tokens:4568\n",
      "characters:29060\n"
     ]
    }
   ],
   "source": [
    "# enumerate returns a key, element tuple, the x[1] is the DocMetaInfo(doc)\n",
    "# https://stackoverflow.com/questions/16945518/finding-the-index-of-the-value-which-is-the-min-or-max-in-python/16945868#16945868\n",
    "idx_of_max_token, doc_meta = max(enumerate(docs_meta_list), key=lambda x: x[1].token_size)\n",
    "print(doc_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2e99d56-f3c6-4fbf-a0fd-713bcf84f9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
       "name:KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
       "tokens:4568\n",
       "characters:29060"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_meta_list[idx_of_max_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c439bf3-c495-40ab-ba4a-33540e594fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230,\n",
       " source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00182^0054877490^2021-06-28^KIIHORMO.txt\n",
       " name:KK-SCIVIAS-00182^0054877490^2021-06-28^KIIHORMO.txt\n",
       " tokens:516\n",
       " characters:3496)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(enumerate(docs_meta_list), key=lambda x: x[1].token_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75663609-b6b2-44d7-ae6d-752cee0c26fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_idx = 1\n",
    "file_idx = idx_of_max_token\n",
    "show_content = False\n",
    "# show_content = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08ff4c4c-7dab-4088-bd83-61511e65ec94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total objects: 250\n",
      "====================\n",
      "s3 key     :s3://scivias-medreports/trans2en/KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
      "obj name   :KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
      "token size :4568\n",
      "char. size :29060\n"
     ]
    }
   ],
   "source": [
    "print_s3_obj_info(data, file_idx, show_content=show_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbf91c-2933-416e-940e-43b2ea414c5c",
   "metadata": {},
   "source": [
    "### Langchain text splitter\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b65bcf1f-813f-481a-9e50-d9ebea07483a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 3000,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "433b8ab8-07a8-4956-bd12-8d9352e590ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "2354\n",
      "2845\n",
      "2800\n",
      "2845\n",
      "2568\n",
      "2889\n",
      "2753\n",
      "2567\n",
      "2415\n",
      "2223\n",
      "2781\n"
     ]
    }
   ],
   "source": [
    "# Optional test of RecursiveCharacterTextSplitter on \\n and other chars\n",
    "test_text = data[file_idx].page_content\n",
    "text_split_list = text_splitter.split_text(test_text)\n",
    "print(len(text_split_list))\n",
    "for seg in text_split_list:\n",
    "    print(len(seg))\n",
    "# print(text_split_list[-1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90cbd4-e5fe-48d6-859d-0444a6e66d1b",
   "metadata": {},
   "source": [
    "### Langchain embeddings\n",
    "\n",
    "use sentence-transformers  \n",
    "\n",
    "* all-MiniLM-L12-v2 : 134MB https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 \n",
    "* all-MiniLM-L6-v2 : 90MB https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main\n",
    "\n",
    "Llama2 does not support document embedding by default\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n",
    "\n",
    "HuggingFaceEmbedding embed_documents example\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "In-memory vectorstore need DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "TextEmbeddings in LangChain\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "Sentence-transformers\n",
    "* https://medium.com/@madhur.prashant7/demo-langchain-rag-demo-on-llama-2-7b-embeddings-model-using-chainlit-559c10ce3fbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee02e3af-d40f-4e1f-a9d9-f8fb36572d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7d63645-9f9e-4bbf-b766-37e5077082bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers/all-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "embed_model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "print(embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a756bf90-97cd-433d-adf8-891cf631b2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5df4e30-6ded-4be1-a6e3-20af0c674ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {'device': 'cpu'}\n",
    "# model_kwargs = {'device_map': \"auto\",}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# is downloaded at \"{MODEL_CACHE_DIR}/models/torch/sentence_transformer\" folder\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f36b2c16-5064-4f71-ba9d-77f154154011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 384)\n"
     ]
    }
   ],
   "source": [
    "test_docs_list = [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    "\n",
    "def embed_vec_dim(embeddings):\n",
    "    return len(embeddings), len(embeddings[0])\n",
    "\n",
    "def embed_docs_test(model: HuggingFaceEmbeddings, docs_list: list):\n",
    "    embeddings = model.embed_documents(\n",
    "        docs_list\n",
    "    )\n",
    "    return embeddings\n",
    "    len(embeddings), len(embeddings[0])\n",
    "\n",
    "embeddings = embed_docs_test(embed_model, test_docs_list)\n",
    "print(embed_vec_dim(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "81c39112-3189-434c-8a57-324afa45d6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 384)\n"
     ]
    }
   ],
   "source": [
    "# print(text_split_list)\n",
    "\n",
    "embeddings = embed_docs_test(embed_model, text_split_list)\n",
    "print(embed_vec_dim(embeddings))\n",
    "# print(embeddings[0])\n",
    "# print(embeddings[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e02332-535e-4a00-88b3-3281e69ab372",
   "metadata": {},
   "source": [
    "## Langchain local LLM RAG\n",
    "\n",
    "Langchain Vectorstore and RAG approach differences:\n",
    "* https://github.com/langchain-ai/langchain/issues/5328\n",
    "\n",
    "Langchain RetrievalQA \n",
    "* https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "\n",
    "DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "LLama2 doesn't support Doc Embedding\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9679882f-495e-4262-8d5c-6883997ba053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAG one document\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embed_model,\n",
    "    text_splitter=text_splitter,\n",
    "    ).from_documents([data[file_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db3042e6-eeca-4129-9360-f34568f9b24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = index.vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e9489af-6dd6-4b82-8060-73f2da0ad5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# db = DocArrayInMemorySearch.from_documents(\n",
    "#     [data[file_idx]], embed_model)\n",
    "\n",
    "# retriever = db.as_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a7eec-2d15-4960-84e1-13ae3d40a130",
   "metadata": {},
   "source": [
    "#### Set the custom template\n",
    "\n",
    "Use the object variable, instead of kwargs\n",
    "https://github.com/langchain-ai/langchain/issues/6635#issuecomment-1659343109\n",
    "\n",
    "The reduce_prompt_template can be set\n",
    "```shell\n",
    "qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bb62d8e-187a-47db-8f07-48833172225a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following extracted parts of a long document and a question, create a final answer.\\n\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n\\n\\n\n",
    "=========\\n\n",
    "QUESTION: {question}\\n\n",
    "=========\\n\n",
    "{summaries}\\n\n",
    "=========\\n\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "reduce_prompt_template = PromptTemplate(template=template, input_variables=['question', 'summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3459329d-ad92-4c73-8084-ecfab6b1d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "# \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "\n",
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"refine\" \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    verbose=True\n",
    "    )\n",
    "# set the prompt template manually\n",
    "# use the original prompttemplate to do the summary, the current custom template doesn't have the one-short summary example, but just the right format.\n",
    "# qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2fea144d-6296-4d69-91d0-bff4dd84e77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the name of the patient?\"\n",
    "# query = \"What is the weight of the patient?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4fc4954-1334-4332-8789-5bb1f43676af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEBUG=True\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "557a0fff-560f-497e-ab1c-2e6b01945acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c55acbd2-237a-40be-92fb-de2dedfdd9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1473 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain({\"query\": query})\n",
    "#response = qa_chain({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2113a349-9774-4ba7-86fe-9cd9b66f5b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a2436c90-e04b-4a2f-b1aa-894d52a12c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response = index.query_with_sources(query, llm=llm, retriever_kwargs={\"chain_type\":\"map_reduce\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74bf5f-fdad-4843-96c3-95844d7d91fa",
   "metadata": {},
   "source": [
    "### (optional) Additional Read\n",
    "\n",
    "GPT4All\n",
    "* https://python.langchain.com/docs/integrations/llms/gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c03817-da5b-445d-b3cc-ec7cd8a29cf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9fd27939-7b6e-4a5c-884f-c4813825c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861587f9-f41e-4ce3-903b-ec50b17f4cce",
   "metadata": {},
   "source": [
    "#### zero shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b5316fa-82d8-4f47-98fa-9506916a1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name\n",
    "input=f\"Can you tell me the name of the patient from the folowing doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ddd6b804-67bd-4c54-a7c2-0e5ef14aa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input)\n",
    "# 6810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "780a0f12-7d99-4aba-9c8a-5fb1e28f456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3c299015-c5b7-425b-8a79-ba155491c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age\n",
    "input=f\"Can you tell me the age of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9fcf966a-76c1-4e7d-8a6d-0b356b2b86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d4d472f-80db-4d73-ac73-f673bc5fe279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnosis\n",
    "input=f\"Can you tell me the diagnosis of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51d7927d-9cba-4cbf-a05c-4fcaf886c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa11f-0eb2-4dac-aa70-194cfe8e577d",
   "metadata": {},
   "source": [
    "#### Chain-of-thoughts prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name prompt\n",
    "input = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {context}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7881162a-6514-4f31-a64c-60718b2f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e61eea4-3731-4ba5-8c1b-e392cc6fe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "input = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{context}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "# print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca31ec3d-1912-4a07-aab0-c84151783112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "#len(input)\n",
    "# > 6913 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "80fbeb59-1fe4-4ab0-b004-05b2fe2d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a401151-af8b-46f4-8ef2-0177ae075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose prompt\n",
    "input=f\"Context:\\nPatient: Fried is a 34-year-old patient, Diagnoses: Influenza (J09.X2) \\nQuestion:\\nWhat diagnoses has the patient? \\nAnswer:\\nFried is a patient, 34 year-old, has diagnoses Influenza (J09.X2). The answers is Influenza (J09.X2)\\nContext:\\n{context}\\nQuestion:\\nWhat diagnoses has the patient?\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "efd94444-a458-40dd-b95d-2be26f9ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "374b6e3c-3d58-4cf6-b834-1d066519e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "024d74a3-4ef3-4379-9075-a3944812244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
