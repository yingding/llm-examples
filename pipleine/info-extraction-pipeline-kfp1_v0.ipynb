{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df4f1a2-5690-41c6-a061-274545e459a3",
   "metadata": {},
   "source": [
    "# About this Jupyter Notebook\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@updated: 08.09.2023\n",
    "\n",
    "This notebook defines and runs a kubeflow pipeline with KFP python SDK v1 for using LlaMA2 and T5 based de_en translatition models to extract information from a non-structured PDF data.\n",
    "\n",
    "The prompt is specially constructed to extract \"patient name\" and \"patient age\" information from a doctor's letter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cf343-6af6-4ba3-b48d-5985633a45e2",
   "metadata": {},
   "source": [
    "## Install KFP Python SDK to build a V1 pipeline\n",
    "* Build KF pipeline with python SDK: https://www.kubeflow.org/docs/components/pipelines/sdk/build-pipeline/\n",
    "* Current KFP python SDK version on pypi.org: https://pypi.org/project/kfp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3e4c76-43fd-460d-9457-20687bc552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e083866-51d7-400f-9cfd-1790675a6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade --user kfp==2.0.0b13\n",
    "#!{sys.executable} -m pip install --upgrade --user kfp==1.8.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acbec4-7f31-4cbf-bc8a-b852271b4884",
   "metadata": {},
   "source": [
    "## Restart the Kernel\n",
    "\n",
    "After the installation of KFP python SDK, the notebook kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f3811-f3c1-45f2-8476-719a937caadd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting familiar with Jupyter Notebook ENV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5c31cc-0869-48eb-9587-89df6aff7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current platform python version: 3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print (f\"current platform python version: {python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c90732-087d-46de-9a92-c6eb85e8a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                                                         kf-resource-quota\n",
      "Namespace:                                                    kubeflow-kindfor\n",
      "Resource                                                      Used     Hard\n",
      "--------                                                      ----     ----\n",
      "basic-csi.storageclass.storage.k8s.io/persistentvolumeclaims  4        15\n",
      "basic-csi.storageclass.storage.k8s.io/requests.storage        115Gi    150Gi\n",
      "cpu                                                           2300m    128\n",
      "longhorn.storageclass.storage.k8s.io/persistentvolumeclaims   1        15\n",
      "longhorn.storageclass.storage.k8s.io/requests.storage         250Gi    500Gi\n",
      "memory                                                        25518Mi  512Gi\n",
      "requests.nvidia.com/mig-1g.10gb                               0        2\n",
      "requests.nvidia.com/mig-1g.20gb                               0        1\n",
      "requests.nvidia.com/mig-2g.20gb                               1        1\n"
     ]
    }
   ],
   "source": [
    "# run kubectl command line to see the quota in the name space\n",
    "!kubectl describe quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c70007-489a-49d2-baff-eaeaa7a984a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                       1.8.22\n",
      "kfp-pipeline-spec         0.1.16\n",
      "kfp-server-api            1.8.5\n"
     ]
    }
   ],
   "source": [
    "# examing the kfp python sdk version inside a KubeFlow v1.5.1\n",
    "!{sys.executable} -m pip list | grep kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c986797-6418-4f13-a439-7dfe2e6c09a3",
   "metadata": {},
   "source": [
    "## Setup global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bcd262-cbbb-4914-90f0-9c6dc85d5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubeflow-kindfor\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "client = kfp.Client()\n",
    "NAMESPACE = client.get_user_namespace()\n",
    "EXPERIMENT_NAME = 'scivias' # Name of the experiment in the KF webapp UI\n",
    "EXPERIMENT_DESC = 'extract information from doctors letter'\n",
    "PREFIX = \"llm\"\n",
    "DATA_ROOT = \"/mnt\"\n",
    "DATA_SUB_PATH = \"core-kind/yinwang\"\n",
    "DEFAULT_MODEL_TYPE = \"7B\"\n",
    "\n",
    "print(NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f8e8f5-e74d-4128-b6d1-5d0b371a455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings(base_torch_image='harbor-dmz.srv.med.uni-muenchen.de/core-general/ngc:0.0.0')\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Settings:\n",
    "    base_torch_image: str = \"harbor-dmz.srv.med.uni-muenchen.de/core-general/ngc:0.0.0\"\n",
    "\n",
    "    \n",
    "settings = Settings() \n",
    "print(f\"{settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b18f24-d264-4352-aaf0-e1ee8adaadfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating KubeFlow component from python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3226c3-b0d9-4c2b-9f48-2927f6433aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import kfp dsl components\n",
    "import kfp.dsl as dsl\n",
    "from functools import partial\n",
    "from kfp.dsl import (\n",
    "    pipeline,\n",
    "    ContainerOp,\n",
    "    PipelineVolume\n",
    ")\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    create_component_from_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83427c5f-3bdb-40d4-8d44-3fef37d4e1c1",
   "metadata": {},
   "source": [
    "### Create llm inference component\n",
    "\n",
    "#### Subprocess call to pass the nvidia-smi output\n",
    "\n",
    "* Python 3.5 subprocess.run https://stackoverflow.com/questions/4760215/running-shell-command-and-capturing-the-output\n",
    "* https://stackoverflow.com/questions/7681715/whats-the-difference-between-subprocess-popen-and-call-how-can-i-use-them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf024d03-f8aa-486e-8157-1c3c3cc92068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_inference_component.yaml\",\n",
    "    base_image=settings.base_torch_image, # use pt base image\n",
    "    packages_to_install=[\n",
    "        # f\"tensorflow-datasets=={settings.tf_datasets}\",\n",
    "    ] # adding additional libs\n",
    ")\n",
    "def llm_inference(data_root: str, data_sub_path: str, model_type: str, prompt: str) -> str:\n",
    "    import subprocess\n",
    "    import os, time, sys, re\n",
    "    \n",
    "    \n",
    "    class GPUInfoHelper():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "        def byte_gb_info(self, byte_mem) -> str:\n",
    "            \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "            # format the f string float with :.2f to decimal digits\n",
    "            # https://zetcode.com/python/fstring/\n",
    "            return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "        def accelerator_mem_info(self, device_idx: int):\n",
    "            # total\n",
    "            t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "            # usable\n",
    "            r = torch.cuda.memory_reserved(device_idx)\n",
    "            # allocated\n",
    "            a = torch.cuda.memory_allocated(device_idx)\n",
    "            # still free\n",
    "            f = r-a  \n",
    "            print( # \"GPU memory info:\\n\" + \n",
    "                  f\"Physical  memory : {self.byte_gb_info(t)}\\n\" + \n",
    "                  f\"Reserved  memory : {self.byte_gb_info(r)}\\n\" + \n",
    "                  f\"Allocated memory : {self.byte_gb_info(a)}\\n\" + \n",
    "                  f\"Free      memory : {self.byte_gb_info(f)}\")\n",
    "\n",
    "\n",
    "        def accelerator_compute_info(self, device_idx: int) -> None:\n",
    "            name = torch.cuda.get_device_properties(device_idx).name\n",
    "            count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "            print(f\"Device_name      : {name} \\n\" +\n",
    "                  f\"Multi_processor  : {count}\")    \n",
    "\n",
    "\n",
    "        def gpu_usage(self) -> None:        \n",
    "            num_of_gpus = torch.cuda.device_count();\n",
    "            # this shows only the gpu device, not the MIG\n",
    "            print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "            for device_idx in range(torch.cuda.device_count()):\n",
    "                print(\"-\"*20)\n",
    "                self.accelerator_compute_info(device_idx)                 \n",
    "                self.accelerator_mem_info(device_idx)\n",
    "                print(\"-\"*20)\n",
    "    \n",
    "    \n",
    "    def display_container_info():\n",
    "        print(\"-\"*10)\n",
    "        print(f\"python version: {sys.version}\")\n",
    "        print(f\"torch version: {torch.__version__}\")\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "    \n",
    "    def nvidia_device_uuid(input: str):\n",
    "        \"\"\"parse the nvidia devices uuid from the nvidia device info str\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # r'' before the search pattern indicates it is a raw string, \n",
    "            # otherwise \"\" instead of single quote\n",
    "            uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "        except AttributeError:\n",
    "            # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "            uuid = \"\"\n",
    "        return uuid\n",
    "    \n",
    "    \n",
    "    def nvidia_device_info() -> str:\n",
    "        \"\"\"get the nvidia MIGs device uuid and GPU uuid \n",
    "        \"\"\"\n",
    "        # blocking call\n",
    "        result = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE)\n",
    "        # decode the byte object, returns string with \\n\n",
    "        cmd_out_str = result.stdout.decode('utf-8')\n",
    "        return [line.strip() for line in cmd_out_str.split('\\n') if len(line) > 0]\n",
    "        \n",
    "    \n",
    "    def nvidia_mig_uuids() -> str:\n",
    "        \"\"\"get a comma separated str of nvidia MIGs devices\n",
    "        \"\"\"\n",
    "        info_list = nvidia_device_info()\n",
    "        # skip the first GPU ID, get the MIGs IDS\n",
    "        uuid_list = [nvidia_device_uuid(e) for e in info_list[1:]]\n",
    "        # if multi gpus need to join the device together for pytorch\n",
    "        return \",\".join(uuid_list)\n",
    "    \n",
    "    \n",
    "    def init_cuda_torch(uuids: str, data_path: str) -> None:\n",
    "        \"\"\"setup the default env variables for transformers\n",
    "        \n",
    "        Args:\n",
    "          uuids: a comma separate str of nvidia gpu/mig uuids\n",
    "        \"\"\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = uuids \n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "        os.environ['XDG_CACHE_HOME']=f\"{data_path}/models\"\n",
    "        \n",
    "        \n",
    "    def show_folder_files(folder: str) -> None:\n",
    "        print(os.listdir(folder))\n",
    "        \n",
    "        \n",
    "    def huggingface_access_token(data_path: str) -> str:\n",
    "        token_file_path = f\"{data_path}/.cache/huggingface/token\"\n",
    "        token = \"\"\n",
    "        with open(token_file_path, \"r\") as file:\n",
    "            token = file.read().replace('\\n', '')\n",
    "        return token\n",
    "        \n",
    "        \n",
    "    '''Global variable'''\n",
    "    model_map = {\n",
    "        \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"70B\" : \"meta-llama/Llama-2-70b-hf\" \n",
    "    }\n",
    "    data_path = f\"{data_root}/{data_sub_path}\"\n",
    "    model_name = model_map.get(model_type, \"7B\")\n",
    "    nvidia_state = GPUInfoHelper()\n",
    "    \n",
    "    '''Initialization'''\n",
    "    UUIDs = nvidia_mig_uuids()\n",
    "    init_cuda_torch(UUIDs, data_path)\n",
    "    import torch\n",
    "    display_container_info()\n",
    "    print(UUIDs)\n",
    "    \n",
    "    show_folder_files(data_path)\n",
    "    \n",
    "    '''Transformers must be imported after the init_cuda_torch to get env set'''\n",
    "    import transformers\n",
    "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    def chat_gen(\n",
    "        generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "        tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast\n",
    "    ):    \n",
    "        def local(input: str, print_mode: bool = True) -> list:\n",
    "            start = time.time()\n",
    "            sequences = generator(\n",
    "                input,\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                # max_length=200,\n",
    "                max_new_tokens=200,\n",
    "            )\n",
    "            result = []\n",
    "            for seq in sequences:\n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "\n",
    "            end = time.time()\n",
    "            duration = end - start\n",
    "            if print_mode == True:\n",
    "                for s in result:\n",
    "                    print(s)\n",
    "\n",
    "                print(\"-\"*20)\n",
    "                print(f\"walltime: {duration} in secs.\")\n",
    "                gpu_usage() \n",
    "            else:\n",
    "                return result\n",
    "        return local\n",
    "    \n",
    "    \n",
    "    token = huggingface_access_token(data_path)\n",
    "    print(f\"Loading LLM model {model_name} ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "    \n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        token=token,\n",
    "        # use_auth_token=token,\n",
    "    )\n",
    "    # print gpu mem usage after loading the llm\n",
    "    nvidia_state.gpu_usage()\n",
    "    # create convenient chat function\n",
    "    chat = chat_gen(generator, tokenizer)\n",
    "    \n",
    "    talk_back = chat(prompt)\n",
    "    return talk_back\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f1ed-4564-4302-8fb9-094e1325ebe0",
   "metadata": {},
   "source": [
    "### Create data processing component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165590ad-f3ba-482f-bcdc-16635f5ab69c",
   "metadata": {},
   "source": [
    "### Define Helper Function\n",
    "Difference between 2Gi and 2G:\n",
    "* https://stackoverflow.com/questions/50804915/kubernetes-size-definitions-whats-the-difference-of-gi-and-g/50805048#50805048\n",
    "\n",
    "Set MIG GPU requests:\n",
    "* https://github.com/kubeflow/pipelines/issues/6858#issuecomment-1007511676\n",
    "\n",
    "```python\n",
    "containerOp.add_resource_request(gpu_resource, gpu_req)\n",
    "containerOp.add_resource_limit(gpu_resource, gpu_lim)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb43819c-aeb2-4ab0-a395-54dc3c3bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod_resource_transformer(op: ContainerOp, mem_req=\"200Mi\", cpu_req=\"2000m\", mem_lim=\"4000Mi\", cpu_lim='4000m', gpu_req=None, gpu_lim=None):\n",
    "    \"\"\"\n",
    "    this function helps to set the resource limit for container operators\n",
    "    op.set_memory_limit('1000Mi') = 1GB\n",
    "    op.set_cpu_limit('1000m') = 1 cpu core\n",
    "    \"\"\"\n",
    "    gpu_resource = \"nvidia.com/mig-1g.20gb\"\n",
    "    new_op = op.set_memory_request(mem_req)\\\n",
    "        .set_memory_limit(mem_lim)\\\n",
    "        .set_cpu_request(cpu_req)\\\n",
    "        .set_cpu_limit(cpu_lim)\n",
    "    if (gpu_req is not None) and (gpu_lim is not None):\n",
    "        new_op.add_resource_request(gpu_resource, gpu_req)\n",
    "        new_op.add_resource_limit(gpu_resource, gpu_lim)\n",
    "    return new_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30253e28-bf1e-4716-a05b-2524952da9b5",
   "metadata": {},
   "source": [
    "## Define Pipeline\n",
    "* Intro Kubeflow pipeline: https://v1-5-branch.kubeflow.org/docs/components/pipelines/introduction/\n",
    "* Kubeflow pipeline SDK v1: https://v1-5-branch.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f43545d-ddfd-49ad-8f43-fb7600d4b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    "    description = EXPERIMENT_DESC\n",
    ")\n",
    "def custom_pipeline(data_root: str= \"/mnt\", data_sub_path: str=\"core-kind/yinwang\", model_type: str=\"7B\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      data_root: the mount path of shared data volume.\n",
    "      data_sub_path: the relative path to the data folder, without leading ./\n",
    "    \"\"\"\n",
    "    \n",
    "    '''local variable'''\n",
    "    no_artifact_cache = \"P0D\"\n",
    "    artifact_cache_today = \"P1D\"\n",
    "    cache_setting = no_artifact_cache\n",
    "    prompt = \"how are you buddy?\"\n",
    "    \n",
    "    '''Pipeline Volume'''\n",
    "    # predefined pvc in namespace\n",
    "    shared_volume = PipelineVolume(\"llm-models\")\n",
    "    \n",
    "    '''pipeline'''   \n",
    "    inference_task = llm_inference(\n",
    "        data_root=data_root, \n",
    "        data_sub_path=data_sub_path, \n",
    "        model_type=model_type,\n",
    "        prompt=prompt\n",
    "    )\n",
    "    # 200 MB ram and 1 cpu\n",
    "    inference_task = pod_resource_transformer(inference_task, mem_req=\"24000Mi\", cpu_req=\"1000m\", mem_lim=\"24000Mi\", cpu_lim=\"2000m\", gpu_req=1, gpu_lim=1)\n",
    "    # set the download caching to be 1day, disable caching with P0D\n",
    "    inference_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    inference_task.add_pvolumes({data_root: shared_volume})\n",
    "    inference_task.set_display_name(\"llama2 inference\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e789aa-5ccc-4290-8d9d-b3297128417b",
   "metadata": {},
   "source": [
    "### (optional) pipeline compile step\n",
    "use the following command to compile the pipeline to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb19bcb-2215-4ca6-93a8-bd65a31f6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPE_LINE_FILE_NAME=f\"{PREFIX}_kfp1_info_extraction_pipeline\"\n",
    "kfp.compiler.Compiler().compile(custom_pipeline, f\"{PIPE_LINE_FILE_NAME}.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d198c16-f9ab-4e4c-a2f2-d710bec7aba9",
   "metadata": {},
   "source": [
    "### Create Experiment Run\n",
    "\n",
    "create run label with current data time\n",
    "```python\n",
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "ts = datetime.strftime(datetime.now(ptimezone(\"Europe/Berlin\")), \"%Y-%m-%d %H-%M-%S\")\n",
    "print(ts)\n",
    "```\n",
    "\n",
    "Reference:\n",
    "* https://stackoverflow.com/questions/25837452/python-get-current-time-in-right-timezone/25887393#25887393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08dd9330-1a3f-4fcd-8e53-fa7e4a05de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "\n",
    "def get_local_time_str(target_tz_str: str = \"Europe/Berlin\", format_str: str = \"%Y-%m-%d %H-%M-%S\") -> str:\n",
    "    \"\"\"\n",
    "    this method is created since the local timezone is miss configured on the server\n",
    "    @param: target timezone str default \"Europe/Berlin\"\n",
    "    @param: \"%Y-%m-%d %H-%M-%S\" returns 2022-07-07 12-08-45\n",
    "    \"\"\"\n",
    "    target_tz = ptimezone(target_tz_str) # create timezone, in python3.9 use standard lib ZoneInfo\n",
    "    # utc_dt = datetime.now(datetime.timezone.utc)\n",
    "    target_dt = datetime.now(target_tz)\n",
    "    return datetime.strftime(target_dt, format_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1ed73-9fd8-4e59-855e-690e738d09d9",
   "metadata": {},
   "source": [
    "### Config pipeline run\n",
    "* Setting imagePullSecretes for Pipeline with SDK: https://github.com/kubeflow/pipelines/issues/5843#issuecomment-859799181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "853837b3-604d-439b-9cea-4b8e226c0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kubernetes import client as k8s_client\n",
    "pipeline_config = dsl.PipelineConf()\n",
    "\n",
    "# pipeline_config.set_image_pull_secrets([k8s_client.V1ObjectReference(name=K8_GIT_SECRET_NAME, namespace=NAME_SPACE)])\n",
    "pipeline_config.set_image_pull_policy(\"Always\")\n",
    "# pipeline_config.set_image_pull_policy(\"IfNotPresent\")\n",
    "\n",
    "pipeline_args = {\n",
    "    'data_root' : DATA_ROOT,\n",
    "    'data_sub_path' : DATA_SUB_PATH,\n",
    "    'model_type': DEFAULT_MODEL_TYPE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5264dc44-9abd-48a2-970e-3ed8693edfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/f5acd22e-7836-41a6-b8cd-a555a7f71f39\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/f4a6f068-5157-496e-9c16-116ac2281e39\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=f4a6f068-5157-496e-9c16-116ac2281e39)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_NAME = f\"{PREFIX}_extract_info_kfp1 {get_local_time_str()}\"\n",
    "\n",
    "# client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=custom_pipeline,\n",
    "    arguments = pipeline_args, #{}\n",
    "    run_name = RUN_NAME,\n",
    "    pipeline_conf=pipeline_config,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd6d15-c88d-4d5a-9b4e-fb2f3706c2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
